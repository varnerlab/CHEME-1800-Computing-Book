
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Dimensionality Reduction &#8212; CHEME 1800/4800</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://varnerlab.github.io/CHEME-1800-Computing-Book/landing.html/unit-2-data/reduction.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The Learning Problem: Models, Learning, and Optimization" href="../unit-3-learning/learning-landing.html" />
    <link rel="prev" title="Vectors, Matrices and Linear Algebraic Equations" href="vectors-matricies-nla.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-VQRVBL1C02"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-VQRVBL1C02');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/cornell_seal_simple_black.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CHEME 1800/4800</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../landing.html">
                    Principles of Computational Thinking for Engineers
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-1-basics/basics-landing.html">
   Unit 1. Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/types.html">
     Expressions, Variables and Types
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/functions.html">
     Functions, Control Statements, and Recursion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/programs.html">
     Programs and Modules
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/data-file-io.html">
     Data Input and Output
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="data-landing.html">
   Unit 2. Data
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="trees.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vectors-matricies-nla.html">
     Vectors, Matrices and Linear Algebraic Equations
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Dimensionality Reduction
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-3-learning/learning-landing.html">
   Unit 3. Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-learning/penalty.html">
     Ordinary Least Squares Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-learning/lp.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-learning/combitorial.html">
     Dynamic Progamming and Heuristic Optimization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-4-decisions/decisions-landing.html">
   Unit 4. Decisions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-4-decisions/simple.html">
     Probability and Simple Choices under Uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-4-decisions/mdp.html">
     Markov Decision Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-4-decisions/multi-arm-bandits.html">
     Multiple Arm Bandit Problems and Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../References.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/varnerlab/CHEME-1800-Computing-Book/main?urlpath=tree/unit-2-data/reduction.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/varnerlab/CHEME-1800-Computing-Book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/varnerlab/CHEME-1800-Computing-Book/issues/new?title=Issue%20on%20page%20%2Funit-2-data/reduction.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/unit-2-data/reduction.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="../_sources/unit-2-data/reduction.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#measurements-and-distances">
   Measurements and distances
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-and-matrix-norms">
     Vector and matrix norms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-functions">
     Similarity functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#content-dimensionality-reduction">
   Dimensionality reduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalue-eigenvector-problems">
     Eigenvalue-eigenvector problems
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#characteristic-polynomial">
       Characteristic polynomial
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#eigenvectors">
       Eigenvectors
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#methods-to-compute-eigenvalues-and-eigenvectors">
     Methods to compute eigenvalues and eigenvectors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-using-the-eigen-function">
       Example: Using the eigen function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-qr-iteration">
       Example: QR iteration
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#singular-value-decomposition">
     Singular value decomposition
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#structural-decomposition-using-svd">
       Structural decomposition using SVD
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principal-component-analysis-pca">
     Principal component analysis (PCA)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#covariance-matrix">
       Covariance matrix
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Dimensionality Reduction</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#measurements-and-distances">
   Measurements and distances
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-and-matrix-norms">
     Vector and matrix norms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-functions">
     Similarity functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#content-dimensionality-reduction">
   Dimensionality reduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalue-eigenvector-problems">
     Eigenvalue-eigenvector problems
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#characteristic-polynomial">
       Characteristic polynomial
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#eigenvectors">
       Eigenvectors
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#methods-to-compute-eigenvalues-and-eigenvectors">
     Methods to compute eigenvalues and eigenvectors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-using-the-eigen-function">
       Example: Using the eigen function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-qr-iteration">
       Example: QR iteration
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#singular-value-decomposition">
     Singular value decomposition
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#structural-decomposition-using-svd">
       Structural decomposition using SVD
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principal-component-analysis-pca">
     Principal component analysis (PCA)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#covariance-matrix">
       Covariance matrix
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="dimensionality-reduction">
<h1>Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Permalink to this headline">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>In this lecture, we’ll discuss measurements and distances and dimensionality reduction. Measurement and distance tools measure the size of matrix or vector objects and the distances between these objects. We’ll explore two types of measurement and distance approaches:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-measurements-matrix-vector-norms"><span class="std std-ref">Vector and matrix norms</span></a> are mathematical tools to measure the magnitude of matrices and vectors, respectively. They are essential in many areas of mathematics, including linear algebra, optimization, and analysis.</p></li>
<li><p><a class="reference internal" href="#content-measurements-similarity-funtions"><span class="std std-ref">Similarity functions</span></a> are mathematical tools that quantify the similarity between objects, such as vectors or points. They are widely used in machine learning, pattern recognition, and data analysis. One example of a similarity function is the radial basis function, which measures the distance between two points using a Gaussian distribution and is commonly used in clustering and classification algorithms.</p></li>
</ul>
<p>On the other hand, dimensionality reduction techniques reduce the number of variables in a dataset while retaining as much information as possible:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-dimensionality-reduction"><span class="std std-ref">Dimensionality reduction</span></a> systematically reduces the number of variables in a dataset while preserving as much information as possible. Dimensionality reduction simplifies data, removes noise, and makes patterns in the data more visible. It can also help visualize data, improve machine learning algorithms’ performance, and reduce the storage and computational requirements of working with large datasets.</p></li>
</ul>
<hr class="docutils" />
</section>
<section id="measurements-and-distances">
<span id="content-measurements-distances"></span><h2>Measurements and distances<a class="headerlink" href="#measurements-and-distances" title="Permalink to this headline">#</a></h2>
<section id="vector-and-matrix-norms">
<span id="content-measurements-matrix-vector-norms"></span><h3>Vector and matrix norms<a class="headerlink" href="#vector-and-matrix-norms" title="Permalink to this headline">#</a></h3>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norm</a> is a function that measures the length of vectors or matrices. The notion of length is handy because it enables us to define distance, i.e., similarity between vectors (or matrices) in applications such as machine learning.</p>
<p>A vector norm is any function <span class="math notranslate nohighlight">\(||\star||:\mathbb{R}^{n}\rightarrow\mathbb{R}\)</span> such that following properties are true:</p>
<ul class="simple">
<li><p>Non-negativity: <span class="math notranslate nohighlight">\(||\mathbf{x}||\geq{0}\)</span> for any vector <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{n}\)</span></p></li>
<li><p>Multiplication by a scalar: <span class="math notranslate nohighlight">\(||\alpha\mathbf{x}|| = \alpha{||\mathbf{x}||}\)</span> for any vector <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{n}\)</span> and <span class="math notranslate nohighlight">\(\alpha\in\mathbb{R}\)</span>.</p></li>
<li><p>Triangle inequality: <span class="math notranslate nohighlight">\(||\mathbf{x}+\mathbf{x}||\leq||\mathbf{x}||+||\mathbf{x}||\)</span> for any vectors <span class="math notranslate nohighlight">\(\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}\)</span></p></li>
</ul>
<div class="proof definition admonition" id="defn-vector-p-norm">
<p class="admonition-title"><span class="caption-number">Definition 26 </span> (p-norm)</p>
<section class="definition-content" id="proof-content">
<p>Arguably, the most commonly used vector norms belong to the family of <span class="math notranslate nohighlight">\(p\)</span>-norms (also called <span class="math notranslate nohighlight">\(l_{p}\)</span>-norms) which is defined as:</p>
<div class="math notranslate nohighlight" id="equation-eqn-p-norm-defn">
<span class="eqno">(55)<a class="headerlink" href="#equation-eqn-p-norm-defn" title="Permalink to this equation">#</a></span>\[||\mathbf{x}||_{p} = \left(\sum_{i=1}^{n}|x_{i}|^{p}\right)^{1/p}\]</div>
<p>for any <span class="math notranslate nohighlight">\(p&gt;0\)</span>.</p>
</section>
</div><p>A matrix norm is any function <span class="math notranslate nohighlight">\(||\star||:\mathbb{R}^{m\times{n}}\rightarrow\mathbb{R}\)</span> such that following properties are true:</p>
<ul class="simple">
<li><p>Non-negativity: <span class="math notranslate nohighlight">\(||\mathbf{A}||\geq{0}\)</span> for any matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and <span class="math notranslate nohighlight">\(||\mathbf{A}||=0\)</span> if and only if <span class="math notranslate nohighlight">\(\mathbf{A}=0\)</span>.</p></li>
<li><p>Multiplication by a scalar: <span class="math notranslate nohighlight">\(||\alpha\mathbf{A}|| = \alpha{||\mathbf{A}||}\)</span> for any <span class="math notranslate nohighlight">\(m\times{n}\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and <span class="math notranslate nohighlight">\(\alpha\in\mathbb{R}\)</span>.</p></li>
<li><p>Triangle inequality: <span class="math notranslate nohighlight">\(||\mathbf{A}+\mathbf{B}||\leq||\mathbf{A}||+||\mathbf{B}||\)</span> for any matrices <span class="math notranslate nohighlight">\(\mathbf{A},\mathbf{B}\in\mathbb{R}^{m\times{n}}\)</span></p></li>
</ul>
<div class="proof definition admonition" id="defn-compatible-matrix-vector-norm">
<p class="admonition-title"><span class="caption-number">Definition 27 </span> (Properties of Matrix Norms)</p>
<section class="definition-content" id="proof-content">
<p>A matrix norm <span class="math notranslate nohighlight">\(||\cdot||\)</span> is <em>consistent</em> with a vector norm <span class="math notranslate nohighlight">\(||\cdot||\)</span> if:</p>
<div class="math notranslate nohighlight" id="equation-eqn-consistent-matrix-norm">
<span class="eqno">(56)<a class="headerlink" href="#equation-eqn-consistent-matrix-norm" title="Permalink to this equation">#</a></span>\[||\mathbf{A}\mathbf{x}||\leq||\mathbf{A}||\cdot||\mathbf{x}||\]</div>
<p>Further, a matrix norm <span class="math notranslate nohighlight">\(||\cdot||\)</span> is <em>sub-multiplicative</em> if <span class="math notranslate nohighlight">\(\forall\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and
<span class="math notranslate nohighlight">\(\forall\mathbf{B}\in\mathbb{R}^{n\times{q}}\)</span> the inequality holds:</p>
<div class="math notranslate nohighlight" id="equation-eqn-submul-norm">
<span class="eqno">(57)<a class="headerlink" href="#equation-eqn-submul-norm" title="Permalink to this equation">#</a></span>\[||\mathbf{A}\mathbf{B}||\leq||\mathbf{A}||\cdot||\mathbf{B}||\]</div>
</section>
</div></section>
<section id="similarity-functions">
<span id="content-measurements-similarity-funtions"></span><h3>Similarity functions<a class="headerlink" href="#similarity-functions" title="Permalink to this headline">#</a></h3>
<p>Other functions can be used to measure the similarity (or distance between) vectors and matrices:</p>
<div class="proof definition admonition" id="defn-rbf-measure">
<p class="admonition-title"><span class="caption-number">Definition 28 </span> (Radial basis function)</p>
<section class="definition-content" id="proof-content">
<p>A radial basis function (RBF) measures the similarity between two input vectors <span class="math notranslate nohighlight">\(\mathbf{x}_{p}\in\mathbb{R}^{m\times{1}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x_{q}}\in\mathbb{R}^{m\times{1}}\)</span>. The most common is the Gaussian radial basis function, defined by a bell-shaped curve:</p>
<div class="math notranslate nohighlight" id="equation-eqn-rbf-similarity-function">
<span class="eqno">(58)<a class="headerlink" href="#equation-eqn-rbf-similarity-function" title="Permalink to this equation">#</a></span>\[k\left(\mathbf{x}_{p},\mathbf{x}_{q}\right) = \sigma_{f}^{2}\exp\left(-\frac{1}{2}\left(\mathbf{x}_{p} - \mathbf{x}_{q}\right)^{T}\mathbf{M}
\left(\mathbf{x}_{p} - \mathbf{x}_{q}\right)\right) +\sigma_{n}^{2}\delta_{pq}\]</div>
<p>where the matrix <span class="math notranslate nohighlight">\(\mathbf{M}\in\mathbb{R}^{m\times{m}}\)</span> can be any symmetric matrix, <span class="math notranslate nohighlight">\(\sigma_{f}^{2}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{n}^{2}\)</span> are constants and <span class="math notranslate nohighlight">\(\delta_{pq}\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a>.</p>
</section>
</div><p>We shall see that radial basis functions are used in various machine learning applications, such as classification.</p>
</section>
</section>
<section id="content-dimensionality-reduction">
<span id="id1"></span><h2>Dimensionality reduction<a class="headerlink" href="#content-dimensionality-reduction" title="Permalink to this headline">#</a></h2>
<p>Dimensionality reduction tools systematically reduce the number of variables in a dataset while preserving as much of the information in the data as possible. Dimensionality reduction simplifies data, removes noise, and makes patterns in the data more visible. It can also help visualize data, improve machine learning algorithms’ performance, and reduce the storage and computational requirements of working with large datasets.</p>
<p>There are many techniques for dimensionality reduction, including <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a>, which are both related to <a class="reference internal" href="#content-eigenvalue-eigenvector-problems"><span class="std std-ref">Eigenvalue-eigenvector problems</span></a>. Other approaches, such as clustering and <a class="reference external" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-distributed stochastic neighbor embedding (t-SNE)</a>, are based upon minimizing a particular distance measure.</p>
<section id="eigenvalue-eigenvector-problems">
<span id="content-eigenvalue-eigenvector-problems"></span><h3>Eigenvalue-eigenvector problems<a class="headerlink" href="#eigenvalue-eigenvector-problems" title="Permalink to this headline">#</a></h3>
<p>Eigenvalue-eigenvector problems involve finding a set of scalar values <span class="math notranslate nohighlight">\(\left\{\lambda_{1},\dots,\lambda_{m}\right\}\)</span> called <a class="reference external" href="https://mathworld.wolfram.com/Eigenvalue.html">eigenvalues</a> and a set of linearly independent vectors <span class="math notranslate nohighlight">\(\left\{\mathbf{v}_{1},\dots,\mathbf{v}_{m}\right\}\)</span> called <a class="reference external" href="https://mathworld.wolfram.com/Eigenvector.html">eigenvectors</a> such that:</p>
<div class="math notranslate nohighlight" id="equation-eqn-eigenvalue-eigenvector-problem">
<span class="eqno">(59)<a class="headerlink" href="#equation-eqn-eigenvalue-eigenvector-problem" title="Permalink to this equation">#</a></span>\[\mathbf{A}\mathbf{v}_{j} = \lambda_{j}\mathbf{v}_{j}\qquad{j=1,2,\dots,m}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{m}}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}\in\mathbb{R}^{m\times{1}}\)</span>, and <span class="math notranslate nohighlight">\(\lambda\in\mathbb{R}\)</span> is a scalar. Eigenvalues and eigenvectors are used in many areas of mathematics, engineering, and physics, including dynamics, image compression and data reduction:</p>
<ul class="simple">
<li><p><strong>Solution of Differential Equations</strong>: Eigenvalues and eigenvectors are used to solve systems of differential equations. Eigenvectors form a set of linearly independent solutions, while eigenvalues determine the stability of these solutions.</p></li>
<li><p><strong>Structural Analysis</strong>: Eigenvalues and eigenvectors can be used to study the structural properties of a matrix or a graph. For example, in structural engineering, eigenvalues and eigenvectors can be used to analyze a structure’s natural frequencies and modes of vibration, e.g., buildings and bridges.</p></li>
<li><p><strong>Principal Component Analysis (PCA)</strong>: PCA is a statistical technique that reduces the dimensionality of a dataset. It is commonly used in data analysis, computer vision, and image processing. In PCA, the eigenvalues and eigenvectors of the <a class="reference external" href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a> are used to find the most important features of the dataset.</p></li>
</ul>
<p>In addition, eigenvalues have another interesting feature (<a class="reference internal" href="#obs-eigenvalues-determinants">Observation 4</a>):</p>
<div class="proof observation admonition" id="obs-eigenvalues-determinants">
<p class="admonition-title"><span class="caption-number">Observation 4 </span> (Determinants and eigenvalues)</p>
<section class="observation-content" id="proof-content">
<p>Eigenvalues can be used directly to calculate the determinant of a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{m}}\)</span>. Denote the set of
eignenvalues for the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{m}}\)</span> as <span class="math notranslate nohighlight">\(\left\{\lambda_{1},\dots,\lambda_{m}\right\}\)</span>. Then, the <span class="math notranslate nohighlight">\(\det\left(\mathbf{A}\right)\)</span> is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-det-a-eigenvalues">
<span class="eqno">(60)<a class="headerlink" href="#equation-eqn-det-a-eigenvalues" title="Permalink to this equation">#</a></span>\[\det\left(\mathbf{A}\right) = \prod_{i=1}^{m}\lambda_{i}\]</div>
<p>A matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{m}}\)</span> is non-singular if <span class="math notranslate nohighlight">\(\text{abs}(\lambda_{i})&gt;0~\forall{i}\)</span>, otherwise it is singular.</p>
</section>
</div><section id="characteristic-polynomial">
<h4>Characteristic polynomial<a class="headerlink" href="#characteristic-polynomial" title="Permalink to this headline">#</a></h4>
<p>The roots of the characteristic polynomial are the eigenvalues of a square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{n\times{n}}\)</span>. The characteristic polynomial plays a central theoretical role in many areas of mathematics, including linear algebra, differential equations, and dynamical systems theory (<a class="reference internal" href="#defn-characteristic-polynomial">Definition 29</a>):</p>
<div class="proof definition admonition" id="defn-characteristic-polynomial">
<p class="admonition-title"><span class="caption-number">Definition 29 </span> (Characteristic polynomial)</p>
<section class="definition-content" id="proof-content">
<p>The characteristic polynomial of a square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{n\times{n}}\)</span> is a polynomial that is obtained by subtracting the scalar <span class="math notranslate nohighlight">\(\lambda\)</span> from the diagonal of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and computing its determinant:</p>
<div class="math notranslate nohighlight" id="equation-eqn-characteristic-polynomial">
<span class="eqno">(61)<a class="headerlink" href="#equation-eqn-characteristic-polynomial" title="Permalink to this equation">#</a></span>\[\det\left(\mathbf{A}-\lambda\mathbf{I}\right) = 0\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the <span class="math notranslate nohighlight">\(n\times{n}\)</span> identity matrix, and <span class="math notranslate nohighlight">\(\lambda\)</span> is an eigenvalue of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. The characteristic polynomial is a polynomial of degree <span class="math notranslate nohighlight">\(n\)</span> with <span class="math notranslate nohighlight">\(\lambda\)</span> as the variable, and its roots are precisely the eigenvalues of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>.</p>
</section>
</div><p>However, despite its theoretical importance, in applications, we will rarely explicitly solve in the characteristic polynomial for the eigenvalues of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. Instead, we’ll use one of several more accessible techniques, see <a class="reference internal" href="#content-compute-eigenvalues-eigenvectors"><span class="std std-ref">Methods to compute eigenvalues and eigenvectors</span></a>.</p>
</section>
<section id="eigenvectors">
<h4>Eigenvectors<a class="headerlink" href="#eigenvectors" title="Permalink to this headline">#</a></h4>
<p>To compute the eigenvectors of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{n\times{n}}\)</span>, we first need the eigenvalues <span class="math notranslate nohighlight">\(\left\{\lambda_{1},\dots,\lambda_{n}\right\}\)</span>. Once we have the eigenvalues, we rearrange Eqn <a class="reference internal" href="#equation-eqn-eigenvalue-eigenvector-problem">(59)</a> for each eigenvalue <span class="math notranslate nohighlight">\(\lambda_{j}\)</span> to give a system of homogenous linear algebraic equations that can be solved for the associated eigenvector
(<a class="reference internal" href="#defn-homogenous-eigenvector-system">Definition 30</a>):</p>
<div class="proof definition admonition" id="defn-homogenous-eigenvector-system">
<p class="admonition-title"><span class="caption-number">Definition 30 </span> (Eigenvector system)</p>
<section class="definition-content" id="proof-content">
<p>Let the eigenvalues of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{n\times{n}}\)</span> be given by the set <span class="math notranslate nohighlight">\(\left\{\lambda_{1},\dots,\lambda_{n}\right\}\)</span>. Then for each eigenvalue <span class="math notranslate nohighlight">\(\lambda_{j}\)</span>, there exists an eigenvector <span class="math notranslate nohighlight">\(\mathbf{v}_{j}\)</span> that is a solution of the homogenous system of equations:</p>
<div class="math notranslate nohighlight" id="equation-eqn-homogenous-eigenvector-system">
<span class="eqno">(62)<a class="headerlink" href="#equation-eqn-homogenous-eigenvector-system" title="Permalink to this equation">#</a></span>\[\left(\mathbf{A}-\lambda_{j}\mathbf{I}\right)\mathbf{v}_{j} = \mathbf{0}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> denotes the <span class="math notranslate nohighlight">\(n\times{n}\)</span> identity matrix. While eigenvectors are always linearly independent, they are not unique (in any case) and orthogonal only for a symmetric <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>.</p>
</section>
</div></section>
</section>
<section id="methods-to-compute-eigenvalues-and-eigenvectors">
<span id="content-compute-eigenvalues-eigenvectors"></span><h3>Methods to compute eigenvalues and eigenvectors<a class="headerlink" href="#methods-to-compute-eigenvalues-and-eigenvectors" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Power_iteration">Power iteration</a> is an iterative method that starts with a random vector and repeatedly multiplies the matrix by this vector, normalizing the result each time. As the iteration proceeds, the vector converges to the eigenvector corresponding to the largest eigenvalue. This method efficiently computes the dominant eigenvalue and eigenvector of a large, sparse matrix. <a class="reference external" href="https://en.wikipedia.org/wiki/Lanczos_algorithm">Lanczos iteration</a> is similar to power iteration but uses a truncated orthogonalization process to compute a small number of eigenvalues and eigenvectors of a large, sparse matrix.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm">Divide-and-conquer algorithms</a> decompose the matrix into smaller matrices, recursively compute their eigenvalues and eigenvectors, and then combine them to obtain the eigenvalues and eigenvectors of the original matrix. This method is efficient for symmetric matrices.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/QR_algorithm">QR iteration</a> applies a sequence of orthogonal similarity transformations to the matrix, which gradually transforms it into a diagonal matrix with the eigenvalues on the diagonal. The eigenvectors can be computed by back-substitution. This method is more expensive than power iteration but can compute all eigenvalues and eigenvectors of a matrix.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular value decomposition</a> computes a matrix’s singular values and singular vectors, which are related to its eigenvalues and eigenvectors. It is handy for calculating the low-rank approximations of a matrix or for dimensionality reduction.</p></li>
</ul>
<section id="example-using-the-eigen-function">
<h4>Example: Using the eigen function<a class="headerlink" href="#example-using-the-eigen-function" title="Permalink to this headline">#</a></h4>
<p>In <a class="reference external" href="https://julialang.org">Julia</a>, eigenvalues and eigenvectors of a dense matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{n\times{n}}\)</span> can be calculated using the <a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen">eigen</a> function included with the standard <code class="docutils literal notranslate"><span class="pre">LinearAlgebra</span></code> package:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># LinearAlgebra package (included in the SLIB, so no download)</span>
<span class="k">using</span> <span class="n">LinearAlgebra</span>

<span class="c"># Setup matrix the n x n matrix A (n = 3)</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.0</span> <span class="o">-</span><span class="mf">0.3</span> <span class="o">-</span><span class="mf">0.2</span> <span class="p">;</span> <span class="mf">0.1</span> <span class="mf">7.0</span> <span class="o">-</span><span class="mf">0.3</span> <span class="p">;</span> <span class="mf">0.3</span> <span class="o">-</span><span class="mf">0.2</span> <span class="mf">10.0</span><span class="p">];</span>

<span class="c"># Decompose using the built-in function</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">eigen</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>   <span class="c"># eigenvalues and vectors in F of type Eigen</span>
<span class="n">λ</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">values</span><span class="p">;</span>   <span class="c"># vector of eigenvalues</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">vectors</span><span class="p">;</span>  <span class="c"># 3 x 3 matrix of eigenvectors, each col is an eigenvector</span>

<span class="c"># print out the eigenvalues</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Eigenvalues λ = </span><span class="si">$</span><span class="p">(</span><span class="n">λ</span><span class="p">)</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvalues λ = [3.017277143754452, 6.9699146114784165, 10.012808244767134]
</pre></div>
</div>
</div>
</div>
<p>Because eigenvectors are linearly independent, the determinant of the matrix of eigenvectors should be non-zero:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># LinearAlgebra package (included in the SLIB, so no download)</span>
<span class="k">using</span> <span class="n">LinearAlgebra</span>

<span class="c"># Setup matrix the n x n matrix A (n = 3)</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.0</span> <span class="o">-</span><span class="mf">0.3</span> <span class="o">-</span><span class="mf">0.2</span> <span class="p">;</span> <span class="mf">0.1</span> <span class="mf">7.0</span> <span class="o">-</span><span class="mf">0.3</span> <span class="p">;</span> <span class="mf">0.3</span> <span class="o">-</span><span class="mf">0.2</span> <span class="mf">10.0</span><span class="p">];</span>

<span class="c"># Decompose using the built-in function</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">eigen</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>   <span class="c"># eigenvalues and vectors in F of type Eigen</span>
<span class="n">λ</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">values</span><span class="p">;</span>   <span class="c"># vector of eigenvalues</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">vectors</span><span class="p">;</span>  <span class="c"># 3 x 3 matrix of eigenvectors, each col is an eigenvector</span>

<span class="c"># print out the eigenvalues</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;det(V) = </span><span class="si">$</span><span class="p">(</span><span class="n">det</span><span class="p">(</span><span class="n">V</span><span class="p">))</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>det(V) = 0.9913342222326557
</pre></div>
</div>
</div>
</div>
<p>In <a class="reference external" href="https://www.python.org">Python</a>, eigenvalues and eigenvectors can be computed using the <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html">eig</a> function of the <a class="reference external" href="https://numpy.org">NumPy</a> library.</p>
</section>
<section id="example-qr-iteration">
<h4>Example: QR iteration<a class="headerlink" href="#example-qr-iteration" title="Permalink to this headline">#</a></h4>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/QR_algorithm">QR algorithm</a> iteratively calculates the eigenvalues and eigenvectors of a square matrix. The <a class="reference external" href="https://en.wikipedia.org/wiki/QR_algorithm">QR algorithm</a>, independently developed in the late 1950s by <a class="reference external" href="https://en.wikipedia.org/wiki/John_G._F._Francis">John G. F. Francis</a> and by <a class="reference external" href="https://en.wikipedia.org/wiki/Vera_Kublanovskaya">Vera N. Kublanovskaya</a>, is one of the ten algorithms of the twentieth century <span id="id2">[<a class="reference internal" href="../References.html#id3" title="J. Dongarra and F. Sullivan. Guest editors introduction to the top 10 algorithms. Computing in Science &amp; Engineering, 2(01):22-23, jan 2000. doi:10.1109/MCISE.2000.814652.">Dongarra and Sullivan, 2000</a>]</span>.</p>
<div class="proof definition admonition" id="defn-qr-iteration">
<p class="admonition-title"><span class="caption-number">Definition 31 </span> (Eigenvalues and Eigenvectors using QR iteration)</p>
<section class="definition-content" id="proof-content">
<p>The basic idea is to perform <a class="reference external" href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> repeatedly, writing the matrix as a product of an orthogonal matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> and an upper triangular matrix <span class="math notranslate nohighlight">\(\mathbf{R}\)</span>, multiply the factors in the reverse order, and iterate.</p>
<p><strong>Phase 1: Eigenvalues</strong>. Let <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{n\times{n}}\)</span>. Then, at the kth step of the procedure (starting with <span class="math notranslate nohighlight">\(k = 0\)</span>), we compute the <a class="reference external" href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eqn-qr-iteration">
<span class="eqno">(63)<a class="headerlink" href="#equation-eqn-qr-iteration" title="Permalink to this equation">#</a></span>\[\mathbf{A}_{k+1}=\mathbf{Q}_{k}\mathbf{R}_{k} = \mathbf{Q}_{k}^{T}\mathbf{A}_{k}\mathbf{Q}_{k}\qquad{k=0,1,\dots} \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Q}_{k}\)</span> is an orthogonal matrix, i.e., <span class="math notranslate nohighlight">\(\mathbf{Q}^{T}_{k} = \mathbf{Q}^{−1}_{k}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{R}_{k}\)</span> is an upper triangular matrix. As <span class="math notranslate nohighlight">\(k\rightarrow\infty\)</span>, the matrix <span class="math notranslate nohighlight">\(\mathbf{A}_{k}\)</span> converge to a triangular matrix where eigenvalues are listed along the diagonal.</p>
<p><strong>Phase 2: Eigenvectors</strong>. Once we’ve calculated the eigenvalues <span class="math notranslate nohighlight">\(\left\{\lambda_{1},\dots,\lambda_{n}\right\}\)</span>, we can compute the eigenvectors associated with each of the eigenvalues <span class="math notranslate nohighlight">\(\left\{\mathbf{v}_{1},\dots,\mathbf{v}_{n}\right\}\)</span>  by solving the homogenous system of equations:</p>
<div class="math notranslate nohighlight">
\[\left(\mathbf{A}-\lambda_{j}\mathbf{I}\right)\mathbf{v}_{j} = \mathbf{0}\]</div>
</section>
</div><p>Let’s do an example (<a class="reference internal" href="#example-QR-iteration">Example 20</a>) where we compute the eigenvalues and eigenvectors of a square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> using the <a class="reference external" href="https://en.wikipedia.org/wiki/QR_algorithm">QR iteration algorithm</a> outlined in <a class="reference internal" href="#defn-qr-iteration">Definition 31</a>.</p>
<div class="proof example dropdown admonition" id="example-QR-iteration">
<p class="admonition-title"><span class="caption-number">Example 20 </span> (QR iteration to compute Eigenvalues and Eigenvectors)</p>
<section class="example-content" id="proof-content">
<p>Compute the eigenvalues and eigenvectors of the matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A} = \begin{bmatrix}
3.0 &amp; -0.3 &amp; -0.2 \\
0.1 &amp; 7.0 &amp; -0.3 \\
0.3 &amp; -0.2 &amp; 10.0 \\
\end{bmatrix}
\end{split}\]</div>
<p>using the <a class="reference external" href="https://en.wikipedia.org/wiki/QR_algorithm">QR iteration algorithm</a> outlined in <a class="reference internal" href="#defn-qr-iteration">Definition 31</a>. How well does your answer compare to the values generated by <a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen">eigen</a> function?</p>
<p><strong>Solution</strong>: Compute the eigenvalues and eigenvectors using the <a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen">Julia eigen function</a> and then using the <code class="docutils literal notranslate"><span class="pre">qriteration</span></code> function (assumed to be included in the workspace):</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">LinearAlgebra</span>

<span class="c"># Setup matrix the n x n matrix A (n = 3)</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.0</span> <span class="o">-</span><span class="mf">0.3</span> <span class="o">-</span><span class="mf">0.2</span> <span class="p">;</span> <span class="mf">0.1</span> <span class="mf">7.0</span> <span class="o">-</span><span class="mf">0.3</span> <span class="p">;</span> <span class="mf">0.3</span> <span class="o">-</span><span class="mf">0.2</span> <span class="mf">10.0</span><span class="p">];</span>

<span class="c"># Decompose using the built-in function</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">eigen</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>   <span class="c"># eigenvalues and vectors in F of type Eigen</span>
<span class="n">λ</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">values</span><span class="p">;</span>   <span class="c"># vector of eigenvalues</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">vectors</span><span class="p">;</span>  <span class="c"># 3 x 3 matrix of eigenvectors, each col is an eigenvector</span>

<span class="c"># Call our qriteration function (assumed to be included in the workspace)</span>
<span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">V</span><span class="p">)</span> <span class="o">=</span> <span class="n">qriteration</span><span class="p">(</span><span class="n">A</span><span class="p">;</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">);</span>

<span class="c"># compare the difference between eigenvalues</span>
<span class="n">δ</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">L</span> <span class="o">-</span> <span class="n">λ</span><span class="p">);</span> <span class="c"># see lecture notes on vector norms</span>
</pre></div>
</div>
<p>The difference <span class="math notranslate nohighlight">\(\delta\)</span> between our <code class="docutils literal notranslate"><span class="pre">qriteration</span></code> implementation and <code class="docutils literal notranslate"><span class="pre">eigen</span></code> was <span class="math notranslate nohighlight">\(\delta = 2.25e-6\)</span>. Thus, our function produces eigenvalues that are close to <code class="docutils literal notranslate"><span class="pre">eigen</span></code>,  with an error of the same magnitude as the <code class="docutils literal notranslate"><span class="pre">tolerance</span></code> parameter.</p>
<p><strong>Source</strong>: The implementation of our <code class="docutils literal notranslate"><span class="pre">qriteration</span></code> function can be found on <a class="reference external" href="https://github.com/varnerlab/CHEME-1800-4800-Course-Repository-S23/tree/main/examples/unit-2-examples/qr">GitHub</a>.</p>
</section>
</div></section>
</section>
<section id="singular-value-decomposition">
<h3>Singular value decomposition<a class="headerlink" href="#singular-value-decomposition" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular value decomposition (SVD)</a> is a powerful tool used in many applications, such as image and data compression, signal processing, and machine learning. Singular value decomposition factors a matrix into the product of an orthogonal matrix, a diagonal matrix, and another orthogonal matrix (<a class="reference internal" href="#defn-svd-real-matrix">Definition 32</a>):</p>
<div class="proof definition admonition" id="defn-svd-real-matrix">
<p class="admonition-title"><span class="caption-number">Definition 32 </span> (Singular value decomposition)</p>
<section class="definition-content" id="proof-content">
<p>Let the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span>. The singular value decomposition of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-math-svd">
<span class="eqno">(64)<a class="headerlink" href="#equation-eqn-math-svd" title="Permalink to this equation">#</a></span>\[\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{U}\in\mathbb{R}^{n\times{n}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{V}\in\mathbb{R}^{m\times{m}}\)</span> are orthogonal matrices and <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\in\mathbb{R}^{n\times{m}}\)</span> is a diagonal matrix containing the singular values <span class="math notranslate nohighlight">\(\sigma_{i}=\Sigma_{ii}\)</span> along the main diagonal.</p>
<p>The columns of <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> are called left-singular vectors, while the columns of <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> are called right-singular vectors.
Singular vectors have a unique property: unlike eigenvectors, left- and right-singular vectors are linearly independent and orthogonal.</p>
</section>
</div><p>The singular value decomposition and eigendecomposition have important connections:</p>
<ul class="simple">
<li><p>Singular values and eigenvalues are related: <span class="math notranslate nohighlight">\(\sigma_{i} = \sqrt\lambda_{i}\)</span></p></li>
<li><p>The columns of <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> (left-singular vectors) are eigenvectors of the matrix product <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^{T}\)</span>.</p></li>
<li><p>The columns of <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> (right-singular vectors) are eigenvectors of the matrix product <span class="math notranslate nohighlight">\(\mathbf{A}^{T}\mathbf{A}\)</span>.</p></li>
</ul>
<section id="structural-decomposition-using-svd">
<h4>Structural decomposition using SVD<a class="headerlink" href="#structural-decomposition-using-svd" title="Permalink to this headline">#</a></h4>
<p>Let’s explore another interesting use of singular value decomposition, namely structural decomposition of a matrix (<a class="reference internal" href="#obs-svd-matrix-decomposition">Observation 5</a>):</p>
<div class="proof observation admonition" id="obs-svd-matrix-decomposition">
<p class="admonition-title"><span class="caption-number">Observation 5 </span> (SVD structural decomposition)</p>
<section class="observation-content" id="proof-content">
<p>Singular value decomposition (SVD) decomposes a rectangular matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> into a weighted, ordered sum of separable matrices.
Let <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> have the singular value decomposition <span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}\)</span>.</p>
<p>Then, the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> can be re-written as:</p>
<div class="math notranslate nohighlight" id="equation-eqn-matrix-decomp">
<span class="eqno">(65)<a class="headerlink" href="#equation-eqn-matrix-decomp" title="Permalink to this equation">#</a></span>\[\mathbf{A} = \sum_{i=1}^{R_{\mathbf{A}}}\sigma_{i}\left(\mathbf{u}_{i}\otimes\mathbf{v}_{i}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(R_{\mathbf{A}}\)</span> is the rank of matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, the vectors <span class="math notranslate nohighlight">\(\mathbf{u}_{i}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_{i}\)</span> are the ith left and right singular vectors, respectively, and <span class="math notranslate nohighlight">\(\sigma_{i}\)</span> are the ordered singular values.</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Outer_product">outer-product</a> <span class="math notranslate nohighlight">\(\left(\mathbf{u}_{i}\otimes\mathbf{v}_{i}\right)\)</span> is the separable component of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. For more details on computing the <a class="reference external" href="https://en.wikipedia.org/wiki/Outer_product">outer-product</a>, see <a class="reference internal" href="vectors-matricies-nla.html#content-vector-vector-operations"><span class="std std-ref">Vector-vector operations</span></a>.</p>
</section>
</div><!-- * The principal components obtained from PCA can be expressed as a linear combination of the columns of the matrix $\mathbf{U}$ obtained from SVD. Additionally, the singular values are proportional to the eigenvalues of the covariance matrix, which provide information about the variance each principal component explains.  -->
</section>
</section>
<section id="principal-component-analysis-pca">
<span id="content-compute-pca"></span><h3>Principal component analysis (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Permalink to this headline">#</a></h3>
<p>Principal Component Analysis (PCA) is a statistical technique that reduces the dimensionality of a dataset while retaining as much of its variation as possible. PCA transforms the original dataset into a new set of uncorrelated variables called principal components, ranked by importance.</p>
<ul class="simple">
<li><p>Principal components are the directions in which the data varies the most and are the eigenvectors of the
<a class="reference external" href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a> of the data.</p></li>
<li><p>PCA is a specific application of SVD to the <a class="reference external" href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a> of a dataset. The principal components are obtained from columns of the matrix <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>. The amount of variance explained by each component is obtained from the singular values in <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>.</p></li>
</ul>
<section id="covariance-matrix">
<h4>Covariance matrix<a class="headerlink" href="#covariance-matrix" title="Permalink to this headline">#</a></h4>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a> is a square matrix that summarizes the pairwise relationships between variables in a dataset. The diagonal elements represent each variable’s <a class="reference external" href="https://en.wikipedia.org/wiki/Variance">variance</a>, i.e., the standard deviation squared. In contrast, the off-diagonal elements represent the <a class="reference external" href="https://en.wikipedia.org/wiki/Covariance">covariance</a> between pairs of variables (<a class="reference internal" href="#defn-covariance matrix">Definition 33</a>):</p>
<div class="proof definition admonition" id="defn-covariance matrix">
<p class="admonition-title"><span class="caption-number">Definition 33 </span> (Covariance matrix)</p>
<section class="definition-content" id="proof-content">
<p>Suppose we have a collection of <span class="math notranslate nohighlight">\(n\)</span>-dimensional random vectors <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Then, <span class="math notranslate nohighlight">\(\mathbf{K}_{XX}\)</span> is the <span class="math notranslate nohighlight">\(n\times{n}\)</span> covariance matrix:</p>
<div class="math notranslate nohighlight" id="equation-eqn-cov-matrix-1">
<span class="eqno">(66)<a class="headerlink" href="#equation-eqn-cov-matrix-1" title="Permalink to this equation">#</a></span>\[\mathbf{K}_{XX} = \text{cov}(\mathbf{X},\mathbf{X}) = 
\mathbb{E}\left[\left(\mathbf{X}-\mathbb{E}(\mathbf{X})\right)\cdot\left(\mathbf{X}-\mathbb{E}(\mathbf{X})\right)^{T}\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{E}(\mathbf{X})\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Expected_value">expected value</a>, e.g., the mean of the random vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. The entries of the <a class="reference external" href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a> <span class="math notranslate nohighlight">\(\mathbf{K}_{XX}\)</span> are given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}k_{ij} = \begin{cases}
i = j &amp; \sigma_{i}^{2} \\
i\neq{j} &amp; \rho_{ij}\sigma_{i}\sigma_{j}
\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_{i}\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Standard_deviation">standard-deviation</a> of variable <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(\rho_{ij}\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Correlation">correlation</a> between variables <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>.</p>
</section>
</div></section>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h2>
<p>In this lecture, we discussed distance measurements and dimensionality reduction. Measurement and distance tools measure the size of matrix or vector objects and the distances between these objects. We explored two types of measurement and distance approaches:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-measurements-matrix-vector-norms"><span class="std std-ref">Vector and matrix norms</span></a> are mathematical tools to measure the magnitude of matrices and vectors, respectively. They are essential in many areas of mathematics, including linear algebra, optimization, and analysis.</p></li>
<li><p><a class="reference internal" href="#content-measurements-similarity-funtions"><span class="std std-ref">Similarity functions</span></a> are mathematical tools that quantify the similarity between objects, such as vectors or points. They are widely used in machine learning, pattern recognition, and data analysis. One example of a similarity function is the radial basis function, which measures the distance between two points using a Gaussian distribution and is commonly used in clustering and classification algorithms.</p></li>
</ul>
<p>Dimensionality reduction techniques reduce the number of variables in a dataset while retaining as much information as possible:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-dimensionality-reduction"><span class="std std-ref">Dimensionality reduction</span></a> systematically reduces the number of variables in a dataset while preserving as much information as possible. Dimensionality reduction simplifies data, removes noise, and makes patterns in the data more visible. It can also help visualize data, improve machine learning algorithms’ performance, and reduce the storage and computational requirements of working with large datasets.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "julia-1.8"
        },
        kernelOptions: {
            kernelName: "julia-1.8",
            path: "./unit-2-data"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'julia-1.8'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="vectors-matricies-nla.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Vectors, Matrices and Linear Algebraic Equations</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../unit-3-learning/learning-landing.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Learning Problem: Models, Learning, and Optimization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Varner<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>