
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Markov Decision Processes &#8212; CHEME 1800/4800</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://varnerlab.github.io/CHEME-1800-Computing-Book/landing.html/unit-4-decisions/mdp.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Multiple Arm Bandit Problems and Reinforcement Learning" href="multi-arm-bandits.html" />
    <link rel="prev" title="Probability and Uncertain Choices" href="simple.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-VQRVBL1C02"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-VQRVBL1C02');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/cornell_seal_simple_black.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CHEME 1800/4800</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../landing.html">
                    Principles of Computational Thinking for Engineers
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-1-basics/basics-landing.html">
   Unit 1. Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/types.html">
     Expressions, Variables and Types
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/functions.html">
     Functions, Control Statements, and Recursion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/programs.html">
     Programs and Modules
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/data-file-io.html">
     Data Input and Output
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-2-data/data-landing.html">
   Unit 2. Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-2-data/trees.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-2-data/vectors-matricies-nla.html">
     Vectors, Matrices and Linear Algebraic Equations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-2-data/reduction.html">
     Dimensionality Reduction
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-3-learning/learning-landing.html">
   Unit 3. Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-learning/penalty.html">
     Ordinary Least Squares Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-learning/lp.html">
     Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-learning/combitorial.html">
     Dynamic Progamming and Heuristic Optimization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="decisions-landing.html">
   Unit 4. Decisions
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="simple.html">
     Probability and Uncertain Choices
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Markov Decision Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multi-arm-bandits.html">
     Multiple Arm Bandit Problems and Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../References.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/varnerlab/CHEME-1800-Computing-Book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/varnerlab/CHEME-1800-Computing-Book/issues/new?title=Issue%20on%20page%20%2Funit-4-decisions/mdp.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/unit-4-decisions/mdp.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Markov Decision Processes
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-chains">
     Markov chains
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#discrete-time-markov-chains">
       Discrete-time Markov chains
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sampling-the-stationary-distribution">
         Sampling the stationary distribution
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hidden-markov-models-hmms">
       Hidden Markov Models (HMMs)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-decision-processes-mdps">
     Markov decision processes (MDPs)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#policy-evaluation">
       Policy evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#value-function-policies">
       Value function policies
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#value-iteration">
       Value iteration
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Markov Decision Processes</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Markov Decision Processes
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-chains">
     Markov chains
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#discrete-time-markov-chains">
       Discrete-time Markov chains
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sampling-the-stationary-distribution">
         Sampling the stationary distribution
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hidden-markov-models-hmms">
       Hidden Markov Models (HMMs)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-decision-processes-mdps">
     Markov decision processes (MDPs)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#policy-evaluation">
       Policy evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#value-function-policies">
       Value function policies
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#value-iteration">
       Value iteration
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="markov-decision-processes">
<h1>Markov Decision Processes<a class="headerlink" href="#markov-decision-processes" title="Permalink to this headline">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>A Markov decision process (MDP) provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. MDPs take their name from the Russian mathematician <a class="reference external" href="https://en.wikipedia.org/wiki/Andrey_Markov">Andrey Markov</a>,  as they are an extension of <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>, a stochastic model which describes a sequence of possible events in which the probability of each event depends only on the system state in the previous event.</p>
<p>In this lecture:</p>
<ul class="simple">
<li><p>We will discuss <a class="reference internal" href="#content-references-markov-chains"><span class="std std-ref">Markov chains</span></a> and discrete time <a class="reference internal" href="#content-references-structure-of-an-hmm"><span class="std std-ref">Hidden Markov Models (HMMs)</span></a>, which are approaches for modeling the evolution of a stochastic system as a series of possible events.</p></li>
<li><p>We will also discuss <a class="reference internal" href="#content-references-structure-of-an-mdp"><span class="std std-ref">Markov decision processes (MDPs)</span></a>, which is an approach for making decisions in a probabilistic world.</p></li>
</ul>
<hr class="docutils" />
</section>
<section id="markov-chains">
<span id="content-references-markov-chains"></span><h2>Markov chains<a class="headerlink" href="#markov-chains" title="Permalink to this headline">#</a></h2>
<p>A Markov chain is a stochastic model describing a sequence of possible events where the probability of each of these events depends only on the system’s current state and not on past system states. A system’s state space and time (much like a probability) can be either discrete or continuous; for most of the applications we’ll be interested in, we’ll focus on discrete time and discrete finite state spaces.</p>
<section id="discrete-time-markov-chains">
<span id="content-references-discrete-time-markov-chains"></span><h3>Discrete-time Markov chains<a class="headerlink" href="#discrete-time-markov-chains" title="Permalink to this headline">#</a></h3>
<figure class="align-default" id="fig-discrete-markov-model">
<a class="reference internal image-reference" href="../_images/Fig-Discrete-MarkovChain-Schematic.pdf"><img alt="../_images/Fig-Discrete-MarkovChain-Schematic.pdf" src="../_images/Fig-Discrete-MarkovChain-Schematic.pdf" style="height: 120px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Schematic of a discrete two-state time-invariant Markov model; <span class="math notranslate nohighlight">\(p_{ij}\)</span> denotes the time-invariant transition probability between state <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>.</span><a class="headerlink" href="#fig-discrete-markov-model" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A discrete-time Markov chain is a sequence of random variables <span class="math notranslate nohighlight">\(X_{1}\)</span>, <span class="math notranslate nohighlight">\(X_{2}\)</span>, <span class="math notranslate nohighlight">\(X_{3}\)</span>, …, <span class="math notranslate nohighlight">\(X_{n}\)</span> that have the <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a>, i.e., the probability of moving to the <em>next state</em> depends only on the <em>present state</em> and not on the <em>previous states</em>:</p>
<div class="math notranslate nohighlight" id="equation-eqn-markov-property">
<span class="eqno">(93)<a class="headerlink" href="#equation-eqn-markov-property" title="Permalink to this equation">#</a></span>\[P(X_{n+1} = x | X_{1}=x_{1}, \dots, X_{n}=x_{n}) = P(X_{n+1} = x | X_{n}=y)\]</div>
<p>where <em>states</em> refer to a finite set of discrete values in which the system can exist.  If the state space is finite, the transition probability distribution, i.e., the probability of moving from the state(s) <span class="math notranslate nohighlight">\(i\rightarrow{j}\)</span>, can be encoded in the transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>. Elements of <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>, denoted as <span class="math notranslate nohighlight">\(p_{ij}\)</span>, encode the probability of moving from state <span class="math notranslate nohighlight">\(i\rightarrow{j}\)</span> during the next time step:</p>
<div class="math notranslate nohighlight" id="equation-eqn-transition-prob-matrix">
<span class="eqno">(94)<a class="headerlink" href="#equation-eqn-transition-prob-matrix" title="Permalink to this equation">#</a></span>\[p_{ij} = P(X_{n+1}~=~j~|~X_{n}~=~i)\]</div>
<p>The transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> has interesting properties:</p>
<ul class="simple">
<li><p>First, the rows of transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> must sum to unity, i.e., each row encodes the probability of all possible outcomes. Thus, it must sum to one.</p></li>
<li><p>Second, if the transition matrix  <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is time-invariant, then <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is the same at each step, i.e., <span class="math notranslate nohighlight">\(p_{ij}\)</span> doesn’t change as <span class="math notranslate nohighlight">\(n\rightarrow{n+1}~\forall{n}\)</span>.</p></li>
</ul>
<p>Putting these ideas together gives (<a class="reference internal" href="#defn-n-transition">Definition 58</a>):</p>
<div class="proof definition admonition" id="defn-n-transition">
<p class="admonition-title"><span class="caption-number">Definition 58 </span> (Time-invariant state transition)</p>
<section class="definition-content" id="proof-content">
<p>A Markov chain has finite state set <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> and the time-invariant state transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>. Let the state vector at time <span class="math notranslate nohighlight">\(j\)</span> be given by <span class="math notranslate nohighlight">\(\mathbf{x}_{j}\)</span>, where <span class="math notranslate nohighlight">\(x_{s,j}\geq{0},\forall{s}\in\mathcal{S}\)</span> and:</p>
<div class="math notranslate nohighlight">
\[\sum_{s\in\mathcal{S}}x_{s,j} = 1\qquad\forall{j}\]</div>
<p>Then, the state of the Markov chain at time step <span class="math notranslate nohighlight">\(n+1\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}_{n+1} = \mathbf{x}_{n}\mathbf{P}^n\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span> denotes the system state vector at time step <span class="math notranslate nohighlight">\(n\)</span>.</p>
</section>
</div><p>Finally, suppose that a Markov chain is both time-invariant and non-periodic. Then, there exists a unique stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{P}^{k}\)</span> converges to a rank-one matrix in which each row is the stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[\lim_{k\rightarrow\infty} \mathbf{P}^{k} = \mathbf{1}\pi\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is a column vector of all 1’s. Let’s consider an example to make these ideas less abstract (<a class="reference internal" href="#example-dicrete-mchain">Example 28</a>):</p>
<div class="proof example dropdown admonition" id="example-dicrete-mchain">
<p class="admonition-title"><span class="caption-number">Example 28 </span> (Discrete Markov chain stationary distribution)</p>
<section class="example-content" id="proof-content">
<p>Consider the time-invariant two-state Discrete Markov chain with state transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{P} = \begin{bmatrix}
0.9 &amp; 0.1 \\
0.6 &amp; 0.4 \\
\end{bmatrix}
\end{split}\]</div>
<p>shown in (<a class="reference internal" href="#fig-discrete-markov-model"><span class="std std-numref">Fig. 18</span></a>). This transition matrix admits a stationary (non-periodic) solution. As the number of iterations <span class="math notranslate nohighlight">\(n\)</span> becomes large the system state converges to a stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span>. Thus, regardless of the starting state of this Markov chain, the long-term behavior is given by the stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>Develop a script to compute the stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># load package -</span>
<span class="k">using</span> <span class="n">LinearAlgebra</span>

<span class="s">&quot;&quot;&quot;</span>
<span class="s">    iterate(P::Array{Float64,2}, counter::Int; </span>
<span class="s">        maxcount::Int = 100, ϵ::Float64 = 0.1) -&gt; Array{Float64,2}</span>

<span class="s">Recursively computes a stationary distribution. </span>
<span class="s">Computation stops if ||P_new - P|| &lt; ϵ or the max number of iterations is hit. </span>
<span class="s">&quot;&quot;&quot;</span>
<span class="k">function</span> <span class="n">iterate</span><span class="p">(</span><span class="n">P</span><span class="o">::</span><span class="kt">Array</span><span class="p">{</span><span class="kt">Float64</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span> <span class="n">counter</span><span class="o">::</span><span class="kt">Int</span><span class="p">;</span> <span class="n">maxcount</span><span class="o">::</span><span class="kt">Int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">ϵ</span><span class="o">::</span><span class="kt">Float64</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span><span class="o">::</span><span class="kt">Array</span><span class="p">{</span><span class="kt">Float64</span><span class="p">,</span><span class="mi">2</span><span class="p">}</span>

    <span class="c"># base case -</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">counter</span> <span class="o">==</span> <span class="n">maxcount</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">P</span>
    <span class="k">else</span>
        <span class="c"># generate a new P -</span>
        <span class="n">P_new</span> <span class="o">=</span> <span class="n">P</span><span class="o">^</span><span class="p">(</span><span class="n">counter</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">err</span> <span class="o">=</span> <span class="n">P_new</span> <span class="o">-</span> <span class="n">P</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">err</span><span class="p">)</span><span class="o">&lt;=</span><span class="n">ϵ</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">P_new</span>
        <span class="k">else</span>
            <span class="c"># we have NOT hit the error target, or the max iterations</span>
            <span class="n">iterate</span><span class="p">(</span><span class="n">P_new</span><span class="p">,</span> <span class="p">(</span><span class="n">counter</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">maxcount</span><span class="o">=</span><span class="n">maxcount</span><span class="p">,</span> <span class="n">ϵ</span> <span class="o">=</span> <span class="n">ϵ</span><span class="p">)</span>
        <span class="k">end</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="c"># Setup the transition probability matrix -</span>
<span class="n">P</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mf">0.9</span> <span class="mf">0.1</span><span class="p">;</span>
    <span class="mf">0.6</span> <span class="mf">0.4</span><span class="p">;</span>
<span class="p">];</span>

<span class="c"># compute -</span>
<span class="n">counter</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">P_stationary</span> <span class="o">=</span> <span class="n">iterate</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">counter</span><span class="p">;</span>  <span class="n">maxcount</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">ϵ</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</section>
</div><section id="sampling-the-stationary-distribution">
<h4>Sampling the stationary distribution<a class="headerlink" href="#sampling-the-stationary-distribution" title="Permalink to this headline">#</a></h4>
<p>Once the stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span> of the Markov chain has been estimated, we can use it to generate samples from that chain.
For example, the stationary distribution for the two-state Markov chain in <a class="reference internal" href="#example-dicrete-mchain">Example 28</a> is given by:</p>
<div class="math notranslate nohighlight">
\[\pi = (0.857,0.143)\]</div>
<p>We model this system as a <a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical distribution</a>. A categorical distribution is a discrete probability distribution that models the probability of a random variable taking on one of a finite set of possible outcomes:</p>
<div class="math notranslate nohighlight" id="equation-eqn-categorical-dist">
<span class="eqno">(95)<a class="headerlink" href="#equation-eqn-categorical-dist" title="Permalink to this equation">#</a></span>\[P(X = k) = \pi\left[k\right]\]</div>
<p>Sampling a categorical distribution constructed from the stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span> will return the stationary
distribution (<a class="reference internal" href="#example-categorical-dist">Example 29</a>):</p>
<div class="proof example dropdown admonition" id="example-categorical-dist">
<p class="admonition-title"><span class="caption-number">Example 29 </span> (Categorical distribution)</p>
<section class="example-content" id="proof-content">
<p>Sample a categorical distribution constructed using the stationary distribution:</p>
<div class="math notranslate nohighlight">
\[\pi = (0.857,0.143)\]</div>
<p>Generate <span class="math notranslate nohighlight">\(N = 1000\)</span> samples and compute the fraction of state <code class="docutils literal notranslate"><span class="pre">1</span></code> and state <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># include -</span>
<span class="n">include</span><span class="p">(</span><span class="s">&quot;Include.jl&quot;</span><span class="p">)</span>

<span class="c"># setup -</span>
<span class="nb">π</span> <span class="o">=</span> <span class="p">[</span> <span class="mf">0.857</span><span class="p">,</span> <span class="mf">0.143</span><span class="p">];</span>
<span class="n">number_of_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">;</span>

<span class="c"># build a categorical distribution </span>
<span class="n">d</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="nb">π</span><span class="p">);</span>

<span class="c"># sample -</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">number_of_samples</span><span class="p">)</span>

<span class="c"># how many 1&#39;s -</span>
<span class="n">number_of_1</span> <span class="o">=</span> <span class="n">findall</span><span class="p">(</span><span class="n">x</span><span class="o">-&gt;</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span> <span class="o">|&gt;</span> <span class="n">length</span><span class="p">;</span>
<span class="n">number_of_2</span> <span class="o">=</span> <span class="n">findall</span><span class="p">(</span><span class="n">x</span><span class="o">-&gt;</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span> <span class="o">|&gt;</span> <span class="n">length</span><span class="p">;</span>

<span class="c"># println -</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;Fraction of state 1: </span><span class="si">$</span><span class="p">(</span><span class="n">number_of_1</span><span class="o">/</span><span class="n">number_of_samples</span><span class="p">)</span><span class="s"> and state 2: </span><span class="si">$</span><span class="p">(</span><span class="n">number_of_2</span><span class="o">/</span><span class="n">number_of_samples</span><span class="p">)</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</div></section>
</section>
<section id="hidden-markov-models-hmms">
<span id="content-references-structure-of-an-hmm"></span><h3>Hidden Markov Models (HMMs)<a class="headerlink" href="#hidden-markov-models-hmms" title="Permalink to this headline">#</a></h3>
<p>Hidden Markov models (HMMs) are statistical models in which the system being modeled is assumed to be a Markov process with unobservable states <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span> but observable outcomes <span class="math notranslate nohighlight">\(o\in\mathcal{O}\)</span>. HMMs have the same structural components as a standard Markov chain model. However, each hidden state can be considered sending an observable single, with the emission probability.</p>
<figure class="align-default" id="fig-discrete-hidden-markov-model">
<a class="reference internal image-reference" href="../_images/Fig-HMM-Schematic-23.pdf"><img alt="../_images/Fig-HMM-Schematic-23.pdf" src="../_images/Fig-HMM-Schematic-23.pdf" style="height: 280px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Schematic of a discrete two-state time-invariant hidden Markov model (HMM); <span class="math notranslate nohighlight">\(p_{ij}\)</span> denotes the time-invariant transition probability between state <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> while <span class="math notranslate nohighlight">\(t_{ij}\)</span> denotes the emission probability for state <span class="math notranslate nohighlight">\(i\)</span> and observation <span class="math notranslate nohighlight">\(j\)</span>.</span><a class="headerlink" href="#fig-discrete-hidden-markov-model" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The emission probability refers to the likelihood of observing a particular output <span class="math notranslate nohighlight">\(Y = o_{t}\)</span>, given the current state of the Markov chain <span class="math notranslate nohighlight">\(X = s_{t}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eqn-hmm-output">
<span class="eqno">(96)<a class="headerlink" href="#equation-eqn-hmm-output" title="Permalink to this equation">#</a></span>\[P(Y = o_{t} | X = s_{t})\]</div>
<p>Similar to the transition probability, the emission probability must sum to unity:</p>
<div class="math notranslate nohighlight">
\[\sum_{o\in\mathcal{O}} P(Y = o | X = s) = 1\qquad\forall{s\in\mathcal{S}}\]</div>
<p>The emission probability plays a crucial role in HMMs, as it is used to calculate the likelihood of a sequence of observed symbols, given the current state of the hidden Markov chain. This likelihood is then used in various applications, including speech recognition, natural language processing, and bioinformatics. The emission probability can be computed using different methods, including maximum likelihood estimation or Bayesian inference.</p>
<p>Let’s build upon <a class="reference internal" href="#example-dicrete-mchain">Example 28</a> and construct an HMM that mimics a <a class="reference external" href="https://en.wikipedia.org/wiki/Trinomial_tree">trinomial lattice model of Boyle</a> (<a class="reference internal" href="#example-dicrete-mchain-hmm">Example 30</a>):</p>
<div class="proof example dropdown admonition" id="example-dicrete-mchain-hmm">
<p class="admonition-title"><span class="caption-number">Example 30 </span> (Stationary hidden Markov model)</p>
<section class="example-content" id="proof-content">
<p>Model the share price of a stock <code class="docutils literal notranslate"><span class="pre">XYZ</span></code> using a time-invariant two-state Discrete Markov chain with the state transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{P} = \begin{bmatrix}
0.60 &amp; 0.40 \\
0.35 &amp; 0.65 \\
\end{bmatrix}
\end{split}\]</div>
<p>and an emission probability matrix <span class="math notranslate nohighlight">\(\mathbf{E}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{E} = \begin{bmatrix}
0.70 &amp; 0.20 &amp; 0.1 \\
0.10 &amp; 0.20 &amp; 0.7 \\
\end{bmatrix}
\end{split}\]</div>
<p>Let <span class="math notranslate nohighlight">\(Y_{i}\)</span> denote the observable value for state <span class="math notranslate nohighlight">\(i\)</span>. Assume output state <code class="docutils literal notranslate"><span class="pre">1</span></code> is an up move (a 1% increase), state <code class="docutils literal notranslate"><span class="pre">2</span></code> the price stays the same, and state <code class="docutils literal notranslate"><span class="pre">3</span></code> indicates a price drop (a 1% decrease).</p>
<p>Simulation script:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># include the include -</span>
<span class="n">include</span><span class="p">(</span><span class="s">&quot;Include.jl&quot;</span><span class="p">);</span> <span class="c"># load paths, packages and codes</span>

<span class="c"># PHASE 1: Setup the calculation</span>
<span class="c"># Setup/initialize</span>
<span class="n">number_of_hidden_states</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">number_of_output_states</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">number_of_simulation_steps</span> <span class="o">=</span> <span class="mi">480</span>
<span class="n">emission_probability_dict</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">{</span><span class="kt">Int</span><span class="p">,</span><span class="kt">Categorical</span><span class="p">}()</span>
<span class="n">simulation_dict</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">{</span><span class="kt">Int</span><span class="p">,</span><span class="kt">Int</span><span class="p">}()</span>

<span class="c"># Transition matrix for the MC</span>
<span class="n">P</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mf">0.60</span> <span class="mf">0.40</span><span class="p">;</span>
    <span class="mf">0.35</span> <span class="mf">0.65</span><span class="p">;</span>
<span class="p">];</span>

<span class="c"># Emission probability matrix -</span>
<span class="n">EPM</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mf">0.70</span> <span class="mf">0.20</span> <span class="mf">0.1</span> <span class="p">;</span>
    <span class="mf">0.10</span> <span class="mf">0.20</span> <span class="mf">0.7</span> <span class="p">;</span>
<span class="p">]</span>

<span class="c"># compute the stationary distribution for the MC -</span>
<span class="nb">π</span> <span class="o">=</span> <span class="n">iterate</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="mi">1</span><span class="p">;</span>  <span class="n">maxcount</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">ϵ</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)[</span><span class="mi">1</span><span class="p">,</span><span class="o">:</span><span class="p">]</span> <span class="c"># assumption: iterate converges</span>
<span class="n">mcd</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="nb">π</span><span class="p">);</span> <span class="c"># Markov-chain distribution</span>

<span class="c"># construct emission probability dictionary -</span>
<span class="k">for</span> <span class="n">i</span> <span class="o">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">number_of_hidden_states</span>
    <span class="n">emission_probability_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">EPM</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="o">:</span><span class="p">])</span>
<span class="k">end</span>

<span class="c"># PHASE 2: Simulation </span>
<span class="k">for</span> <span class="n">i</span> <span class="o">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">number_of_simulation_steps</span>
    
    <span class="c"># which state is the mc in?</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">mcd</span><span class="p">);</span>
    
    <span class="c"># grab the emission probability model from the emission_probability_dict -</span>
    <span class="n">epd</span> <span class="o">=</span> <span class="n">emission_probability_dict</span><span class="p">[</span><span class="n">hidden_state</span><span class="p">];</span>
    
    <span class="c"># role for a random ouput -</span>
    <span class="n">simulation_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">epd</span><span class="p">);</span>
<span class="k">end</span>
</pre></div>
</div>
<p>Plotting script:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># runme -</span>
<span class="n">include</span><span class="p">(</span><span class="s">&quot;runme.jl&quot;</span><span class="p">)</span> <span class="c"># loads packages, computes simulation_dict -</span>

<span class="c"># Setup -</span>
<span class="n">Sₒ</span> <span class="o">=</span> <span class="mf">100.0</span><span class="p">;</span>
<span class="n">Δ</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">];</span>
<span class="n">S</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">{</span><span class="kt">Int</span><span class="p">,</span><span class="kt">Float64</span><span class="p">}()</span>
<span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Sₒ</span>

<span class="c"># setup colors -</span>
<span class="n">colors</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">{</span><span class="kt">Int</span><span class="p">,</span><span class="kt">RGB</span><span class="p">}();</span>
<span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="sa">colorant</span><span class="s">&quot;#0077BB&quot;</span>

<span class="c"># sim loop -</span>
<span class="k">for</span> <span class="n">i</span> <span class="o">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">number_of_simulation_steps</span>

    <span class="c"># market state -</span>
    <span class="n">market_state_index</span> <span class="o">=</span> <span class="n">simulation_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="c"># get previous price -</span>
    <span class="n">S_old</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">];</span>
    <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">S_old</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="n">Δ</span><span class="p">[</span><span class="n">market_state_index</span><span class="p">]);</span>
<span class="k">end</span>

<span class="c"># make a plot -</span>
<span class="n">plot!</span><span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">&quot;&quot;</span><span class="p">)</span>
<span class="n">xlabel!</span><span class="p">(</span><span class="s">&quot;Time index (AU)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">ylabel!</span><span class="p">(</span><span class="s">&quot;Share price (USD/share)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
</pre></div>
</div>
</section>
</div></section>
</section>
<section id="markov-decision-processes-mdps">
<span id="content-references-structure-of-an-mdp"></span><h2>Markov decision processes (MDPs)<a class="headerlink" href="#markov-decision-processes-mdps" title="Permalink to this headline">#</a></h2>
<p>A Markov decision process (MDP) is a way to model decision-making where outcomes are partly random and partly controlled by a decision-maker who receives a reward or penalty for each decision (<a class="reference internal" href="#defn-formal-mdp">Definition 59</a>):</p>
<div class="proof definition admonition" id="defn-formal-mdp">
<p class="admonition-title"><span class="caption-number">Definition 59 </span> (Markov decision tuple )</p>
<section class="definition-content" id="proof-content">
<p>A Markov decision process is the tuple <span class="math notranslate nohighlight">\(\left(\mathcal{S}, \mathcal{A}, R_{a}\left(s, s^{\prime}\right), T_{a}\left(s,s^{\prime}\right), \gamma\right)\)</span> where:</p>
<ul class="simple">
<li><p>The state space <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is the set of all possible states <span class="math notranslate nohighlight">\(s\)</span> that a system can exist in</p></li>
<li><p>The action space <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is the set of all possible actions <span class="math notranslate nohighlight">\(a\)</span> that are available to the agent, where <span class="math notranslate nohighlight">\(\mathcal{A}_{s} \subseteq \mathcal{A}\)</span> is the subset of the action space <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> that is accessible from state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>An expected immediate reward <span class="math notranslate nohighlight">\(R_{a}\left(s, s^{\prime}\right)\)</span> is received after transitioning from state <span class="math notranslate nohighlight">\(s\rightarrow{s}^{\prime}\)</span> due to action <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p>The transition <span class="math notranslate nohighlight">\(T_{a}\left(s,s^{\prime}\right) = P(s_{t+1} = s^{\prime}~|~s_{t}=s,a_{t} = a)\)</span> denotes the probability that action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> will result in state <span class="math notranslate nohighlight">\(s^{\prime}\)</span> at time <span class="math notranslate nohighlight">\(t+1\)</span></p></li>
<li><p>The quantity <span class="math notranslate nohighlight">\(\gamma\)</span> is a <em>discount factor</em>; the discount factor is used to weight the <em>future expected utility</em>.</p></li>
</ul>
<p>Finally, a policy function <span class="math notranslate nohighlight">\(\pi\)</span> is the (potentially probabilistic) mapping from states <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span> to actions <span class="math notranslate nohighlight">\(a\in\mathcal{A}\)</span> used by the agent to solve the decision task.</p>
</section>
</div><p>At each time step, the system is in state <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span> and the decision maker may choose any action <span class="math notranslate nohighlight">\(a\in\mathcal{A}\)</span> that are available in state <span class="math notranslate nohighlight">\(s\)</span>. The system responds at the next time step by <em>potentially</em> moving into a new state <span class="math notranslate nohighlight">\(s^{\prime}\)</span> and rewarding the decision maker <span class="math notranslate nohighlight">\(R_{a}\left(s, s^{\prime}\right)\)</span>. The probability that the system moves into a new state <span class="math notranslate nohighlight">\(s^{\prime}\)</span> depends upon the chosen action <span class="math notranslate nohighlight">\(a\)</span> and the current state <span class="math notranslate nohighlight">\(s\)</span>; this probability is governed by a state transition function <span class="math notranslate nohighlight">\(P_{a}\left(s,s^{\prime}\right)\)</span>.</p>
<section id="policy-evaluation">
<h3>Policy evaluation<a class="headerlink" href="#policy-evaluation" title="Permalink to this headline">#</a></h3>
<p>When determining the best policy for a decision problem, we first need to understand what a policy function <span class="math notranslate nohighlight">\(\pi\)</span> is and how to evaluate it. This process is known as policy evaluation. We calculate the expected utility gained by executing a policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> from state <span class="math notranslate nohighlight">\(s\)</span> and denote it as <span class="math notranslate nohighlight">\(U^{\pi}(s)\)</span>. An optimal policy function <span class="math notranslate nohighlight">\(\pi^{\star}\)</span> maximizes the expected utility and can be defined as:</p>
<div class="math notranslate nohighlight">
\[\pi^{\star}\left(s\right) = \text{arg max}~U^{\pi}(s)\]</div>
<p>for all <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span>. To determine the utility of a policy <span class="math notranslate nohighlight">\(\pi\)</span>, we can iteratively calculate it. If the agent makes a single move, the utility is the reward received by implementing policy <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[U_{1}^{\pi}(s) = R(s,\pi(s))\]</div>
<p>However, if the agent performs two, three, or <span class="math notranslate nohighlight">\(k\)</span> possible iterations, we use a lookahead equation that relates the utility value at iteration <span class="math notranslate nohighlight">\(k\)</span> to <span class="math notranslate nohighlight">\(k+1\)</span>:</p>
<div class="math notranslate nohighlight">
\[U_{k+1}^{\pi}(s) = R(s,\pi(s)) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s,\pi(s))U_{k}^{\pi}(s^{\prime})\]</div>
<p>As <span class="math notranslate nohighlight">\(k\rightarrow\infty\)</span> the lookahead utility converges to a stationary value <span class="math notranslate nohighlight">\(U^{\pi}(s)\)</span> (<a class="reference internal" href="#defn-policy-evalution">Definition 60</a>):</p>
<div class="proof definition admonition" id="defn-policy-evalution">
<p class="admonition-title"><span class="caption-number">Definition 60 </span> (Value function)</p>
<section class="definition-content" id="proof-content">
<p>Suppose we have a Markov decision process with the tuple <span class="math notranslate nohighlight">\(\left(\mathcal{S}, \mathcal{A}, R_{a}\left(s, s^{\prime}\right), T_{a}\left(s,s^{\prime}\right), \gamma\right)\)</span>. Then, the utility of the policy function <span class="math notranslate nohighlight">\(\pi\)</span> equals:</p>
<div class="math notranslate nohighlight" id="equation-eqn-converged-policy-eval">
<span class="eqno">(97)<a class="headerlink" href="#equation-eqn-converged-policy-eval" title="Permalink to this equation">#</a></span>\[U^{\pi}(s) = R(s,\pi(s)) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s, \pi(s))U^{\pi}(s^{\prime})\]</div>
<p>The utility associated with an optimal policy <span class="math notranslate nohighlight">\(\pi^{\star}\)</span> is called the optimal utility <span class="math notranslate nohighlight">\(U^{\star}\)</span>.</p>
</section>
</div><p>Let’s do an example to illustrate policy evaluation (<a class="reference internal" href="#example-MDP-line">Example 31</a>):</p>
<div class="proof example dropdown admonition" id="example-MDP-line">
<p class="admonition-title"><span class="caption-number">Example 31 </span> (Tiger problem)</p>
<section class="example-content" id="proof-content">
<figure class="align-default" id="fig-linear-mdp-schematic">
<a class="reference internal image-reference" href="../_images/Fig-Linear-MDP-Schematic.pdf"><img alt="../_images/Fig-Linear-MDP-Schematic.pdf" src="../_images/Fig-Linear-MDP-Schematic.pdf" style="height: 110px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">Schematic of the Tiger problem modeled as an N-state, two-action Markov decision process. A tiger hides behind the red door while freedom awaits behind the green door.</span><a class="headerlink" href="#fig-linear-mdp-schematic" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>An agent trapped in a long hallway with two doors at either end (<a class="reference internal" href="#fig-linear-mdp-schematic"><span class="std std-numref">Fig. 20</span></a>). Behind the red door is a tiger (and certain death), while behind the green door is freedom. If the agent opens the red door, the agent is eaten (and receives a large negative reward). However, if the agent opens the green door, it escapes and gets a positive reward.</p>
<p>For this problem, the MDP has the tuple components:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{S} = \left\{1,2,\dots,N\right\}\)</span> while the action set is <span class="math notranslate nohighlight">\(\mathcal{A} = \left\{a_{1},a_{2}\right\}\)</span>; action <span class="math notranslate nohighlight">\(a_{1}\)</span> moves the agent one state to the right, action <span class="math notranslate nohighlight">\(a_{2}\)</span> moves the agent one state to the left.</p></li>
<li><p>The agent receives a reward of +10 for entering state 1 (escapes). However, the agent is penalized -100 for entering state N (eaten by the tiger).  Finally, the agent is not charged to move to adjacent locations.</p></li>
<li><p>Let the probability of correctly executing the action <span class="math notranslate nohighlight">\(a_{j}\)</span> be <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
</ul>
<p>Let’s compute <span class="math notranslate nohighlight">\(U^{\pi}(s)\)</span> for different choices for the policy function <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p><strong>source</strong>: <a class="reference external" href="https://github.com/varnerlab/CHEME-1800-4800-Course-Repository-S23">download the live Jupyter notebook from GitHub</a></p>
</section>
</div></section>
<section id="value-function-policies">
<h3>Value function policies<a class="headerlink" href="#value-function-policies" title="Permalink to this headline">#</a></h3>
<p><a class="reference internal" href="#defn-policy-evalution">Definition 60</a> gives us a method to compute the utility for a particular policy <span class="math notranslate nohighlight">\(U^{\pi}(s)\)</span>.
However, suppose we were given the utility and wanted to estimate the policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> from that utility.
Given a utility <span class="math notranslate nohighlight">\(U\)</span>, we can estimate a policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> using the <span class="math notranslate nohighlight">\(Q\)</span>-function (action-value function):</p>
<div class="math notranslate nohighlight" id="equation-eqn-action-value-function">
<span class="eqno">(98)<a class="headerlink" href="#equation-eqn-action-value-function" title="Permalink to this equation">#</a></span>\[Q(s,a) = R(s,a) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s, a)U(s^{\prime})\]</div>
<p>Equation <a class="reference internal" href="#equation-eqn-action-value-function">(98)</a> gives a <span class="math notranslate nohighlight">\(|\mathcal{S}|\times|\mathcal{A}|\)</span> array (states on the rows, actions on the columns), where the utility is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-utility-from-q">
<span class="eqno">(99)<a class="headerlink" href="#equation-eqn-utility-from-q" title="Permalink to this equation">#</a></span>\[U(s) = \max_{a} Q(s,a)\]</div>
<p>and the policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-eqn-policy-from-q">
<span class="eqno">(100)<a class="headerlink" href="#equation-eqn-policy-from-q" title="Permalink to this equation">#</a></span>\[\pi(s) = \text{arg}\max_{a}Q(s,a)\]</div>
</section>
<section id="value-iteration">
<h3>Value iteration<a class="headerlink" href="#value-iteration" title="Permalink to this headline">#</a></h3>
<p>In the previous section, we saw how we could develop <em>a policy</em> <span class="math notranslate nohighlight">\(\pi(s)\)</span> by looking at the values in the <span class="math notranslate nohighlight">\(Q\)</span>-array. However, this required the utility vector; thus, we needed to hypothesize a policy that may not be the <em>optimal policy</em>. There are two techniques to compute optimal policies, and we’ll explore the simpler of the two, namely <em>value iteration</em>.</p>
<p>In <em>value iteration</em>, the value function (the vector of utility values) is updated iteratively using the <em>Bellman update</em> procedure:</p>
<div class="math notranslate nohighlight">
\[U_{k+1}(s) = \max_{a}\left(R(s,a) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s, a)U_{k}(s^{\prime})\right)\]</div>
<p>This procedure is guaranteed to converge to the optimal utility vector (value function).</p>
<div class="proof example dropdown admonition" id="example-MDP-line-mod">
<p class="admonition-title"><span class="caption-number">Example 32 </span> (Modified Tiger problem)</p>
<section class="example-content" id="proof-content">
<figure class="align-default" id="fig-branched-mdp-schematic-mod">
<a class="reference internal image-reference" href="../_images/Fig-Branched-MDP-Schematic.pdf"><img alt="../_images/Fig-Branched-MDP-Schematic.pdf" src="../_images/Fig-Branched-MDP-Schematic.pdf" style="height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">Schematic of the Tiger problem modeled as an N-state, four-action (left, right, up, down) Markov decision process. The hallway has three types of paths: unobstructed (white, free), mildly obstructed (light gray, small cost), and obstructed (dark gray, large cost).</span><a class="headerlink" href="#fig-branched-mdp-schematic-mod" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>An agent is trapped in a long hallway with two doors at either end (<a class="reference internal" href="#fig-branched-mdp-schematic-mod"><span class="std std-numref">Fig. 21</span></a>). Behind the green door is a tiger (and certain death), while behind the red door is freedom. If the agent opens the green door, the agent is eaten (and receives a large negative reward). However, if the agent opens the red door, it escapes and gets a positive reward.</p>
<p>For this problem, the MDP has the tuple components:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{S} = \left\{1,2,\dots,N\right\}\)</span> while the action set is <span class="math notranslate nohighlight">\(\mathcal{A} = \left\{a_{1},a_{2}, a_{3}, a_{4}\right\}\)</span>; action <span class="math notranslate nohighlight">\(a_{1}\)</span> moves the agent one state to the left, action <span class="math notranslate nohighlight">\(a_{2}\)</span> moves the agent one state to the right, action <span class="math notranslate nohighlight">\(a_{3}\)</span> moves the agent one stop up, and action <span class="math notranslate nohighlight">\(a_{4}\)</span> moves the agent one step down.</p></li>
<li><p>The agent receives a positive reward for entering the red state <span class="math notranslate nohighlight">\(N\)</span> (escapes). However, the agent is penalized for entering the green state <span class="math notranslate nohighlight">\(1\)</span> (eaten by the tiger).  Finally, the agent is not charged to move to adjacent locations if those locations are unobstructed. However, there is a small charge to move through mildly obstructed locations (light gray circles) and a larger charge to move through obstructed areas (dark gray circles).</p></li>
<li><p>Let the probability of correctly executing an action <span class="math notranslate nohighlight">\(a_{j}\in\mathcal{A}\)</span> be <span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
</ul>
<p>Let’s use value iteration to estimate the <em>optimal policy</em> <span class="math notranslate nohighlight">\(\pi^{\star}(s)\)</span></p>
<p><strong>source:</strong> <a class="reference external" href="https://github.com/varnerlab/CHEME-5660-Markets-Mayhem-Example-Notebooks">download the live Jupyter notebook from GitHub</a></p>
</section>
</div></section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h1>
<p>A Markov decision process (MDP) provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. MDPs take their name from the Russian mathematician <a class="reference external" href="https://en.wikipedia.org/wiki/Andrey_Markov">Andrey Markov</a>,  as they are an extension of <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>, a stochastic model which describes a sequence of possible events in which the probability of each event depends only on the system state in the previous event.</p>
<p>In this lecture:</p>
<ul class="simple">
<li><p>We discussed <a class="reference internal" href="#content-references-markov-chains"><span class="std std-ref">Markov chains</span></a> and discrete time <a class="reference internal" href="#content-references-structure-of-an-hmm"><span class="std std-ref">Hidden Markov Models (HMMs)</span></a>, which are approaches for modeling the evolution of a stochastic system as a series of possible events. We developed a hidden Markov model for the nodes in a binomial lattice model.</p></li>
<li><p>We also discussed <a class="reference internal" href="#content-references-structure-of-an-mdp"><span class="std std-ref">Markov decision processes (MDPs)</span></a>, which is an approach for making decisions in a probabilistic world. In particular, we
introduced tools to compute the utility of a decision-making policy and the value iteration approach for estimating optimal decision-making policies.</p></li>
</ul>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./unit-4-decisions"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="simple.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Probability and Uncertain Choices</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="multi-arm-bandits.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Multiple Arm Bandit Problems and Reinforcement Learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Varner<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>