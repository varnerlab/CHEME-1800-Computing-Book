
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Vectors, Matrices, Measurements and Distances &#8212; CHEME 1800/4800</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://varnerlab.github.io/CHEME-1800-Computing-Book/landing.html/unit-2-data/vectors-matricies-nla.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Algebraic Equations" href="laes.html" />
    <link rel="prev" title="Data Structures" href="trees.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-VQRVBL1C02"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-VQRVBL1C02');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/cornell_seal_simple_black.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CHEME 1800/4800</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../landing.html">
                    Principles of Computational Thinking for Engineers
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-1-basics/basics-landing.html">
   Unit 1. Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/types.html">
     Expressions, Variables and Types
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/functions.html">
     Functions, Control Statements, and Recursion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/programs.html">
     Programs and Modules
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-basics/data-file-io.html">
     Data Input and Output
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="data-landing.html">
   Unit 2. Data
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="trees.html">
     Data Structures
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Vectors, Matrices, Measurements and Distances
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="laes.html">
     Linear Algebraic Equations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-3-learning/learning-landing.html">
   Unit 3. Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-learning/penalty.html">
     Ordinary Least Squares
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-learning/combitorial.html">
     Combinatorial Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-learning/lp.html">
     Linear Programming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-4-decisions/decisions-landing.html">
   Unit 4. Decisions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-4-decisions/probability-random-variables.html">
     Probability, Random Variables and Stochastic Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-4-decisions/mdp.html">
     Markov Decision Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-4-decisions/multi-arm-bandits.html">
     Multiple Arm Bandit Problems and Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/varnerlab/CHEME-1800-Computing-Book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/varnerlab/CHEME-1800-Computing-Book/issues/new?title=Issue%20on%20page%20%2Funit-2-data/vectors-matricies-nla.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/unit-2-data/vectors-matricies-nla.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matricies-and-vectors">
   Matricies and vectors
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#special-matrices">
     Special matrices
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diagonal-and-identity-matrices">
       Diagonal and identity matrices
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#triangular-matrices">
       Triangular matrices
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#orthogonal-matrices">
       Orthogonal matrices
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#determinant-and-trace">
   Determinant and Trace
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trace">
     Trace
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rank-and-kernel">
   Rank and kernel
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-and-vector-operations">
   Matrix and vector operations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#addition-and-subtraction">
     Addition and subtraction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scalar-multiplication">
     Scalar multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-vector-matrix-vector-and-matrix-matrix-multiplication">
     Vector-vector, matrix-vector and matrix-matrix multiplication
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vector-vector-multiplication">
       Vector-vector multiplication
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#right-multiplication-of-a-matrix-by-a-vector">
       Right multiplication of a matrix by a vector
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#left-multiplication-of-a-matrix-by-a-vector">
       Left multiplication of a matrix by a vector
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#matrix-matrix-products">
       Matrix-Matrix products
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#measurements-and-distances">
   Measurements and distances
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-and-matrix-norms">
     Vector and matrix norms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-distance-metrics">
     Other distance metrics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimensionality-reduction">
   Dimensionality reduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalue-eigenvector-problems">
     Eigenvalue-eigenvector problems
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#singular-value-decomposition">
     Singular value decomposition
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#connection-of-svd-and-eigendecomposition">
       Connection of SVD and eigendecomposition
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additonal-resources">
   Additonal resources
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Vectors, Matrices, Measurements and Distances</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matricies-and-vectors">
   Matricies and vectors
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#special-matrices">
     Special matrices
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diagonal-and-identity-matrices">
       Diagonal and identity matrices
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#triangular-matrices">
       Triangular matrices
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#orthogonal-matrices">
       Orthogonal matrices
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#determinant-and-trace">
   Determinant and Trace
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trace">
     Trace
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rank-and-kernel">
   Rank and kernel
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-and-vector-operations">
   Matrix and vector operations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#addition-and-subtraction">
     Addition and subtraction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scalar-multiplication">
     Scalar multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-vector-matrix-vector-and-matrix-matrix-multiplication">
     Vector-vector, matrix-vector and matrix-matrix multiplication
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vector-vector-multiplication">
       Vector-vector multiplication
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#right-multiplication-of-a-matrix-by-a-vector">
       Right multiplication of a matrix by a vector
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#left-multiplication-of-a-matrix-by-a-vector">
       Left multiplication of a matrix by a vector
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#matrix-matrix-products">
       Matrix-Matrix products
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#measurements-and-distances">
   Measurements and distances
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-and-matrix-norms">
     Vector and matrix norms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-distance-metrics">
     Other distance metrics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimensionality-reduction">
   Dimensionality reduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalue-eigenvector-problems">
     Eigenvalue-eigenvector problems
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#singular-value-decomposition">
     Singular value decomposition
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#connection-of-svd-and-eigendecomposition">
       Connection of SVD and eigendecomposition
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additonal-resources">
   Additonal resources
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="vectors-matrices-measurements-and-distances">
<h1>Vectors, Matrices, Measurements and Distances<a class="headerlink" href="#vectors-matrices-measurements-and-distances" title="Permalink to this headline">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>Vectors and matrices are widely used in computer science, engineering, and other fields where mathematical modeling is essential.
This lecture will introduce Vectors, Matrices, and some operations defined on these objects:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-references-matrix-vector"><span class="std std-ref">Matricies and vectors</span></a> are one- and two-dimensional arrays of numbers. Vectors often represent quantities with both magnitude and direction, such as displacement, velocity, and acceleration. They can also describe points in space or as coefficients in linear equations. Matrices are typically represented as a grid of numbers, with each matrix element represented by a different cell in the grid. Matrices are often used to describe linear transformations, such as rotations and scaling operations, as well as to represent systems of linear equations. They can also represent data sets, with each row representing a different data point and each column representing a distinct feature.</p></li>
<li><p><a class="reference internal" href="#content-measurements-distances"><span class="std std-ref">Measurements and distances</span></a> are tools to measure distances between matrix and vector objects.</p></li>
<li><p><a class="reference internal" href="#content-dimensionality-reduction"><span class="std std-ref">Dimensionality reduction</span></a> systematically reduces the number of variables in a dataset while preserving as much information as possible. Dimensionality reduction simplifies data, removes noise, and makes patterns in the data more visible. It can also help visualize data, improve machine learning algorithms’ performance, and reduce the storage and computational requirements of working with large datasets.</p></li>
</ul>
<hr class="docutils" />
</section>
<section id="matricies-and-vectors">
<span id="content-references-matrix-vector"></span><h2>Matricies and vectors<a class="headerlink" href="#matricies-and-vectors" title="Permalink to this headline">#</a></h2>
<p>Matrices are two-dimensional rectangular arrays of numbers, widgets, etc with <span class="math notranslate nohighlight">\(m\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{A} = 
\begin{pmatrix}
a_{1,1} &amp; a_{1,2} &amp; \cdots &amp; a_{1,n} \\
a_{2,1} &amp; a_{2,2} &amp; \cdots &amp; a_{2,n} \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
a_{m,1} &amp; a_{m,2} &amp; \cdots &amp; a_{m,n} 
\end{pmatrix}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(a_{ij}\)</span> denotes the <em>element</em> of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> that lives on the <span class="math notranslate nohighlight">\(i\)</span>th row and <span class="math notranslate nohighlight">\(j\)</span>th col. By convention, the row index is always the first subscript while the column index is always listed second.</p>
<div class="proof observation admonition" id="observation-0">
<p class="admonition-title"><span class="caption-number">Observation 1 </span> (Matrix shape)</p>
<section class="observation-content" id="proof-content">
<p>Let the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> be an <span class="math notranslate nohighlight">\(m\times{n}\)</span> array. Then, the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is called:</p>
<ul class="simple">
<li><p><strong>Square</strong>: If <span class="math notranslate nohighlight">\(m=n\)</span>, the matrix is called a <em>square</em> matrix; square matrices have some unique properties (as we shall see later).</p></li>
<li><p><strong>Overdetermined</strong>: If <span class="math notranslate nohighlight">\(m&gt;n\)</span>, the matrix is called an <em>overdetermined</em> matrix; overdetermined matrices have more rows than columns.</p></li>
<li><p><strong>Underdetermined</strong>: If <span class="math notranslate nohighlight">\(m&lt;n\)</span>, the matrix is called an <em>underdetermined</em> matrix; underdetermined matrices have more columns than rows.</p></li>
</ul>
</section>
</div><p>Vectors are a specal type of matrix that is one-dimensional, where <em>elements</em> are arranged as either a single row or single column. For example, a <span class="math notranslate nohighlight">\(m\times{1}\)</span> <em>column</em> vector <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{a} = 
\begin{pmatrix}
a_{1} \\
a_{2} \\
\vdots \\
a_{m}
\end{pmatrix}\end{split}\]</div>
<p>while a <span class="math notranslate nohighlight">\(n\times{1}\)</span> dimensional <em>row</em> vector <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\mathbf{a} = 
\begin{pmatrix}
a_{1} &amp; a_{2} &amp; \cdots &amp; a_{n}
\end{pmatrix}\]</div>
<p>Just like numbers, vectors and matrices can participate in typical mathematical operations, such as addition, subtraction and multiplication, with some small differences. For example, the division operation has a much different meaning for Matrices when compared to numbers.</p>
<section id="special-matrices">
<h3>Special matrices<a class="headerlink" href="#special-matrices" title="Permalink to this headline">#</a></h3>
<p>Special matrices have specific properties or characteristics that make them useful in certain mathematical operations or applications. Some examples of special matrices include identity matrices, diagonal matrices, triangular matrices, and orthogonal matrices. These matrices have specific properties that make them useful in linear algebra, optimization, and other fields.</p>
<p>Let’s discuss a few of these special matrices: <a class="reference internal" href="#content-references-diag-id-matrix"><span class="std std-ref">Diagonal and identity matrices</span></a>, <a class="reference internal" href="#content-references-triag-matrix"><span class="std std-ref">Triangular matrices</span></a> and <a class="reference internal" href="#content-references-orth-matrix"><span class="std std-ref">Orthogonal matrices</span></a></p>
<section id="diagonal-and-identity-matrices">
<span id="content-references-diag-id-matrix"></span><h4>Diagonal and identity matrices<a class="headerlink" href="#diagonal-and-identity-matrices" title="Permalink to this headline">#</a></h4>
<p>A diagonal matrix is a square matrix in which all entries outside the main diagonal are zero. Diagonal matrices are used in matrix algebra to simplify calculations, and they can be created by multiplying an identity matrix by a scalar. The identity matrix is a square matrix with <code class="docutils literal notranslate"><span class="pre">1</span></code> on the diagonal and <code class="docutils literal notranslate"><span class="pre">0</span></code> everywhere else. It represents the identity transformation in linear algebra and is often denoted by the symbol <span class="math notranslate nohighlight">\(\mathbf{I}\)</span>.</p>
</section>
<section id="triangular-matrices">
<span id="content-references-triag-matrix"></span><h4>Triangular matrices<a class="headerlink" href="#triangular-matrices" title="Permalink to this headline">#</a></h4>
<p>A triangular matrix is a square matrix that is either upper or lower triangular (<a class="reference internal" href="#defn-triangular-matrices">Definition 3</a>):</p>
<div class="proof definition admonition" id="defn-triangular-matrices">
<p class="admonition-title"><span class="caption-number">Definition 3 </span> (Triangular matrices)</p>
<section class="definition-content" id="proof-content">
<p>Triangular matrices are square arrays with zero entries below or above the main diagonal. An <span class="math notranslate nohighlight">\(n\times{n}\)</span> lower triangular matrix <span class="math notranslate nohighlight">\(\mathbf{L}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{L} = 
\begin{pmatrix}
l_{11} &amp; 0 &amp; \dots &amp; 0 \\
l_{21} &amp; l_{22} &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
l_{n1} &amp; l_{n2} &amp; \dots &amp; l_{nn}
\end{pmatrix}\end{split}\]</div>
<p>has entries <span class="math notranslate nohighlight">\(l_{ij} = 0\)</span> for <span class="math notranslate nohighlight">\(i&lt;j\)</span> (above the diagonal). On the other hand, an <span class="math notranslate nohighlight">\(n\times{n}\)</span> upper triangular matrix <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{U} = 
\begin{pmatrix}
u_{11} &amp; u_{12} &amp; \dots &amp; u_{nn} \\
0 &amp; u_{22} &amp; \dots &amp; u_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; u_{nn}
\end{pmatrix}\end{split}\]</div>
<p>has entries <span class="math notranslate nohighlight">\(u_{ij} = 0\)</span> for <span class="math notranslate nohighlight">\(i&gt;j\)</span> (below the diagonal).</p>
</section>
</div><p>Triangular matrices are useful in various mathematical and computational contexts. For example, they are easy to invert, can be used to represent linear transformations conveniently, and can be used to solve systems of linear algebraic equations efficiently.</p>
</section>
<section id="orthogonal-matrices">
<span id="content-references-orth-matrix"></span><h4>Orthogonal matrices<a class="headerlink" href="#orthogonal-matrices" title="Permalink to this headline">#</a></h4>
<p>Orthogonal matrices are square matrices whose rows and columns are mutually orthogonal and have unit lengths (<a class="reference internal" href="#defn-orthogonal-matrix">Definition 4</a>):</p>
<div class="proof definition admonition" id="defn-orthogonal-matrix">
<p class="admonition-title"><span class="caption-number">Definition 4 </span> (Orthogonal matrices)</p>
<section class="definition-content" id="proof-content">
<p>An <span class="math notranslate nohighlight">\(n\times{n}\)</span> real matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is an orthogonal matrix if:</p>
<div class="math notranslate nohighlight" id="equation-eqn-orthogonal-matrices">
<span class="eqno">(12)<a class="headerlink" href="#equation-eqn-orthogonal-matrices" title="Permalink to this equation">#</a></span>\[\mathbf{A}^{T}\mathbf{A} = \mathbf{A}\mathbf{A}^{T} = \mathbf{I}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A}^{T}\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Transpose">transpose</a> of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> denotes the <span class="math notranslate nohighlight">\(n\times{n}\)</span> identity matrix.</p>
</section>
</div><p>Orthogonal matrices are used in various fields, such as linear algebra, physics, computer graphics, and statistics.</p>
</section>
</section>
</section>
<section id="determinant-and-trace">
<span id="content-determinant-trace"></span><h2>Determinant and Trace<a class="headerlink" href="#determinant-and-trace" title="Permalink to this headline">#</a></h2>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Determinant">determinant of a matrix</a> is a scalar value that can be computed from a square matrix. <a class="reference external" href="https://en.wikipedia.org/wiki/Determinant">Determinants</a> can be used to determine whether a <a class="reference internal" href="laes.html"><span class="doc std std-doc">matrix is invertible</span></a>, and they also have applications in solving systems of linear equations and calculating volume changes in linear transformations. The <a class="reference external" href="https://en.wikipedia.org/wiki/Determinant">determinant of square matrix</a> <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is defined as (<a class="reference internal" href="#defn-det-A">Definition 5</a>):</p>
<div class="proof definition admonition" id="defn-det-A">
<p class="admonition-title"><span class="caption-number">Definition 5 </span> (Determinant)</p>
<section class="definition-content" id="proof-content">
<p>Consider an <span class="math notranslate nohighlight">\(n\times{n}\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, where <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the entry of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> on row <span class="math notranslate nohighlight">\(i\)</span> and column <span class="math notranslate nohighlight">\(j\)</span>.
Then, the determinant of the square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, denoted as <span class="math notranslate nohighlight">\(\det\left(\mathbf{A}\right)\)</span>, is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-det-a">
<span class="eqno">(13)<a class="headerlink" href="#equation-eqn-det-a" title="Permalink to this equation">#</a></span>\[\det\left(\mathbf{A}\right) = \sum_{\sigma\in{S_{n}}}\text{sign}\left(\sigma\right)\prod_{i=1}^{n}a_{i\sigma_{i}}\]</div>
<p>where <span class="math notranslate nohighlight">\(S_{n}\)</span> denotes the Symmtery group of dimension <span class="math notranslate nohighlight">\(n\)</span>, i.e., the set of all possible permutations of the set <span class="math notranslate nohighlight">\({1,2,\dots,n}\)</span>,
<span class="math notranslate nohighlight">\(\text{sign}\left(\sigma\right)\)</span> equals <code class="docutils literal notranslate"><span class="pre">+1</span></code> if the permutation can be obtained with an even number of exchanges; otherwise <code class="docutils literal notranslate"><span class="pre">-1</span></code>.
Finally, <span class="math notranslate nohighlight">\(a_{i\sigma_{i}}\)</span> denotes the entry of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> on row <span class="math notranslate nohighlight">\(i\)</span>, and column <span class="math notranslate nohighlight">\(\sigma_{i}\)</span>.</p>
</section>
</div><p>Although the determinant in the general case, as described in <a class="reference internal" href="#defn-det-A">Definition 5</a>, is difficult to calculate, the determinat of triangular matrices is easy to compute:</p>
<div class="proof observation admonition" id="obs-determinant-triangular-matrix">
<p class="admonition-title"><span class="caption-number">Observation 2 </span> (Determinant triangular matrix)</p>
<section class="observation-content" id="proof-content">
<p>The determinant of a triangular matrix is equal to the product of its diagonal elements. This is true for both upper triangular <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> and lower <span class="math notranslate nohighlight">\(\mathbf{L}\)</span> triangular matrices. For the upper triangular <span class="math notranslate nohighlight">\(n\times{n}\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>, the determinant is equal to:</p>
<div class="math notranslate nohighlight">
\[\det\left(\mathbf{U}\right) = \prod_{i=1}^{n}u_{ii}\]</div>
<p>while the determinant of the <span class="math notranslate nohighlight">\(n\times{n}\)</span> lower triangular matrix <span class="math notranslate nohighlight">\(\mathbf{L}\)</span>  is given by:</p>
<div class="math notranslate nohighlight">
\[\det\left(\mathbf{L}\right) = \prod_{i=1}^{n}l_{ii}\]</div>
<p><strong>Idea</strong>: One potential strategy to efficiently compute a determinant is perhaps to convert the general <span class="math notranslate nohighlight">\(n\times{n}\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> to a triangular form (by some theoretical approach). However, will the determinant of the original matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and its triangular be the same? We’ll need to wait and see.</p>
</section>
</div><section id="trace">
<h3>Trace<a class="headerlink" href="#trace" title="Permalink to this headline">#</a></h3>
<p>On the other hand, the trace of a matrix is the sum of the diagonal elements of a square matrix (<a class="reference internal" href="#defn-trace-A">Definition 6</a>):</p>
<div class="proof definition admonition" id="defn-trace-A">
<p class="admonition-title"><span class="caption-number">Definition 6 </span> (Trace)</p>
<section class="definition-content" id="proof-content">
<p>Consider an <span class="math notranslate nohighlight">\(n\times{n}\)</span> square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, where we denote the <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the entry of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> on row <span class="math notranslate nohighlight">\(i\)</span> and column <span class="math notranslate nohighlight">\(j\)</span>. Then, the trace of the square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, denoted as <span class="math notranslate nohighlight">\(\text{tr}\left(\mathbf{A}\right)\)</span>, is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-trace-a">
<span class="eqno">(14)<a class="headerlink" href="#equation-eqn-trace-a" title="Permalink to this equation">#</a></span>\[\text{tr}\left(\mathbf{A}\right) = \sum_{i=1}^{n}a_{ii}\]</div>
</section>
</div></section>
</section>
<section id="rank-and-kernel">
<h2>Rank and kernel<a class="headerlink" href="#rank-and-kernel" title="Permalink to this headline">#</a></h2>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)">rank of a matrix</a> is a measure of the number of linearly independent rows or columns. The rank of a matrix <em>r</em> of a <span class="math notranslate nohighlight">\(m\times{n}\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is always less than or equal to the minimum of its number of rows and columns:</p>
<div class="math notranslate nohighlight" id="equation-eqn-rank-inequality">
<span class="eqno">(15)<a class="headerlink" href="#equation-eqn-rank-inequality" title="Permalink to this equation">#</a></span>\[r\leq\min\left(m,n\right)\]</div>
<p>Rank can also be considered a measure of the unique information in a matrix; thus, if there is redundant information (rows or columns that are not linearly independent), a matrix will be less than full rank.</p>
<p>The kernel of a matrix is the set of all solutions to the homogeneous equation <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \mathbf{0}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is the matrix, and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a column vector. The dimension of the kernel is equal to the number of columns in the matrix minus its rank. The kernel is also known as the null space of the matrix.</p>
</section>
<section id="matrix-and-vector-operations">
<span id="content-matrix-vector-operations"></span><h2>Matrix and vector operations<a class="headerlink" href="#matrix-and-vector-operations" title="Permalink to this headline">#</a></h2>
<p>Matrix and vector operations are mathematical procedures that include addition, subtraction, scalar multiplication, and matrix multiplication, which can be used to represent and manipulate linear transformations. Understanding these operations is crucial for many areas of science, engineering, and computer science.</p>
<section id="addition-and-subtraction">
<h3>Addition and subtraction<a class="headerlink" href="#addition-and-subtraction" title="Permalink to this headline">#</a></h3>
<p>Matrices and vectors can be added and subtracted just like scalars quantities with one important caveat, namely, they need to be compatible. Vectors and matrices must be the <em>same dimension</em> to be compatible with addition or subtraction operations.</p>
<p>The addition (or subtraction) operations for matrices or vectors, as well as multiplying these objects by a scalar constant (as we shall see), is done element-wise. Thus, if these objects don’t have the same number of elements, then addition and subtraction operations don’t make sense.</p>
<div class="proof definition admonition" id="obs-same-dimension">
<p class="admonition-title"><span class="caption-number">Definition 7 </span> (Vector addition)</p>
<section class="definition-content" id="proof-content">
<p>Suppose we have two <span class="math notranslate nohighlight">\(m\times{1}\)</span> vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. The sum of these vectors <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{a} + \mathbf{b}\)</span> is the <span class="math notranslate nohighlight">\(m\times{1}\)</span> vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-vector-addition">
<span class="eqno">(16)<a class="headerlink" href="#equation-eqn-vector-addition" title="Permalink to this equation">#</a></span>\[y_{i} = a_{i} + b_{i}\qquad{i=1,\dots,m}\]</div>
<p>The vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> are compatible if they have the same number of elements.</p>
</section>
</div><p>Vector addition is strarighforward to implement, consider <a class="reference internal" href="#algo-vector-addition">Algorithm 2</a>:</p>
<div class="proof algorithm dropdown admonition" id="algo-vector-addition">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Naive vector addition)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Compatible vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span></p>
<p><strong>Outputs</strong> Vector sum <span class="math notranslate nohighlight">\(\mathbf{y}\)</span></p>
<p><strong>Initialize</strong></p>
<ol class="simple">
<li><p>set <span class="math notranslate nohighlight">\(n\leftarrow\text{length}(\mathbf{a})\)</span></p></li>
<li><p>set <span class="math notranslate nohighlight">\(\mathbf{y}\leftarrow\text{zeros}(n)\)</span></p></li>
</ol>
<p><strong>Main</strong></p>
<ol class="simple">
<li><p>for <span class="math notranslate nohighlight">\(i\in{1,\dots,n}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(y_{i}\leftarrow~a_{i} + b_{i}\)</span></p></li>
</ol>
</li>
</ol>
<p><strong>Return</strong> sum vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span></p>
</section>
</div><p><a class="reference internal" href="#algo-vector-addition">Algorithm 2</a> may not be the <em>best</em> way to implement vector (or matrix) addition or subtract operations.v Many modern programming languages and libraries, e.g., <a class="reference external" href="https://numpy.org">the Numpy library in Python</a> or <a class="reference external" href="https://julialang.org">Julia</a>, support <em>vectorization</em>, i.e., special operators that encode element-wise addition, subtraction or other types of element-wise operations without the need to write <code class="docutils literal notranslate"><span class="pre">for</span></code> loops.</p>
<p>Vectorized code typically executes faster than naive implementations such as <a class="reference internal" href="#algo-vector-addition">Algorithm 2</a> because the <em>vectorization</em> takes advantage of advanced techniques to improve performance.</p>
<p>You can use the <code class="docutils literal notranslate"><span class="pre">.+</span></code> operator for element-wise addition, while element-wise subtraction can be encoded with the <code class="docutils literal notranslate"><span class="pre">.-</span></code> operator.</p>
</section>
<section id="scalar-multiplication">
<h3>Scalar multiplication<a class="headerlink" href="#scalar-multiplication" title="Permalink to this headline">#</a></h3>
<p>Multiplying a vector (or a matrix) by a constant is also done element wise.</p>
<div class="proof definition admonition" id="defn-scalar-multiplication">
<p class="admonition-title"><span class="caption-number">Definition 8 </span> (Scalar multiplication)</p>
<section class="definition-content" id="proof-content">
<p>Suppose we have a <span class="math notranslate nohighlight">\(n\times{1}\)</span> vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> and a constant <span class="math notranslate nohighlight">\(c\)</span>. Then the scalar product between a constant <span class="math notranslate nohighlight">\(c\)</span> and the vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, denoted by <span class="math notranslate nohighlight">\(\mathbf{y} = c\mathbf{v}\)</span>, is given by:</p>
<div class="math notranslate nohighlight">
\[y_{i} = cv_{i}\qquad{i=1,2,\dots,n}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_{i}\)</span> and <span class="math notranslate nohighlight">\(v_{i}\)</span> denote the ith component of the product vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, and the input vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>.</p>
<p>Likewise, suppose we have an <span class="math notranslate nohighlight">\(m\times{n}\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and constant <span class="math notranslate nohighlight">\(c\)</span>. Then, the scalar product between a constant <span class="math notranslate nohighlight">\(c\)</span> and the matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, denoted by <span class="math notranslate nohighlight">\(\mathbf{Y} = c\mathbf{X}\)</span>, is given by:</p>
<div class="math notranslate nohighlight">
\[y_{ij} = cx_{ij}\qquad{i=1,2,\dots,m,~j=1,2,\dots,n}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_{ij}\)</span> and <span class="math notranslate nohighlight">\(x_{ij}\)</span> denote the ijth component of the product matrix <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>, and the input matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
</section>
</div></section>
<section id="vector-vector-matrix-vector-and-matrix-matrix-multiplication">
<h3>Vector-vector, matrix-vector and matrix-matrix multiplication<a class="headerlink" href="#vector-vector-matrix-vector-and-matrix-matrix-multiplication" title="Permalink to this headline">#</a></h3>
<section id="vector-vector-multiplication">
<h4>Vector-vector multiplication<a class="headerlink" href="#vector-vector-multiplication" title="Permalink to this headline">#</a></h4>
<p>Inner products: Two compatible vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> can be multiplied together to produce a <em>scalar</em> in an operation called an <a class="reference external" href="https://en.wikipedia.org/wiki/Inner_product_space">inner product</a>:</p>
<div class="proof definition admonition" id="defn-vector-vector-multiplication">
<p class="admonition-title"><span class="caption-number">Definition 9 </span> (Inner product)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{a}\in\mathbb{R}^{m\times{1}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\in\mathbb{R}^{m\times{1}}\)</span>. Then, the <em>vector-vector inner product</em> is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-vector-vector-inner-product">
<span class="eqno">(17)<a class="headerlink" href="#equation-eqn-vector-vector-inner-product" title="Permalink to this equation">#</a></span>\[y = \mathbf{a}^{T}\mathbf{b}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{a}^{T}\)</span> denotes the transpose of the vector <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and the scalar <span class="math notranslate nohighlight">\(y\)</span> equals:</p>
<div class="math notranslate nohighlight" id="equation-eqn-vector-inner-product-index-form">
<span class="eqno">(18)<a class="headerlink" href="#equation-eqn-vector-inner-product-index-form" title="Permalink to this equation">#</a></span>\[y = \sum_{i=1}^{m}a_{i}b_{i}\]</div>
<p>This operation is possible if the vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> have the same number of elements.</p>
</section>
</div><p>Outer product: Suppose you have an <span class="math notranslate nohighlight">\(m\times{1}\)</span> vector <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>, and an <span class="math notranslate nohighlight">\(n\times{1}\)</span> vector <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. The vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> can be multipled together to form a <span class="math notranslate nohighlight">\(m\times{n}\)</span> matrix through an <a class="reference external" href="https://en.wikipedia.org/wiki/Outer_product">outer product</a>:</p>
<div class="proof definition admonition" id="defn-vector-vector-multiplication-op">
<p class="admonition-title"><span class="caption-number">Definition 10 </span> (Outer product)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{a}\in\mathbb{R}^{m\times{1}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\in\mathbb{R}^{n\times{1}}\)</span>. Then, the <em>vector-vector outer product</em> given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-vector-vector-outer-product">
<span class="eqno">(19)<a class="headerlink" href="#equation-eqn-vector-vector-outer-product" title="Permalink to this equation">#</a></span>\[\mathbf{Y} = \mathbf{a}\otimes\mathbf{b}\]</div>
<p>produces the <span class="math notranslate nohighlight">\(m\times{n}\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{m\times{n}}\)</span> with elements:</p>
<div class="math notranslate nohighlight" id="equation-eqn-vector-outer-product-index-form">
<span class="eqno">(20)<a class="headerlink" href="#equation-eqn-vector-outer-product-index-form" title="Permalink to this equation">#</a></span>\[y_{ij} = a_{i}b_{j}\qquad{i=1,2,\dots,m~\text{and}~j=1,2,\dots,n}\]</div>
<p>The outer product operation is equivalent to <span class="math notranslate nohighlight">\(\mathbf{Y} = \mathbf{a}\mathbf{b}^{T}\)</span>.</p>
</section>
</div></section>
<section id="right-multiplication-of-a-matrix-by-a-vector">
<h4>Right multiplication of a matrix by a vector<a class="headerlink" href="#right-multiplication-of-a-matrix-by-a-vector" title="Permalink to this headline">#</a></h4>
<p>A common operation is <em>right matrix-vector multiplication</em> of a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> by a vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. If the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are compatible,  <em>right matrix-vector multiplication</em> will produce a vector with the same number of rows as the original matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>:</p>
<div class="proof definition admonition" id="defn-right-matrix-vector-multiplication">
<p class="admonition-title"><span class="caption-number">Definition 11 </span> (Right matrix-vector multiplication)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{n\times{1}}\)</span>. Then, the <em>right matrix-vector product</em> given by:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{A}\mathbf{x}\]</div>
<p>generates <span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{m\times{1}}\)</span> where the <span class="math notranslate nohighlight">\(i\)</span>th element is given by:</p>
<div class="math notranslate nohighlight">
\[y_{i} = \sum_{j=1}^{n}a_{ij}x_{j}\qquad{i=1,2,\cdots,m}\]</div>
<p>This operation is possible if the number of columns of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> equals the number of rows of the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
</section>
</div><p>The <em>right multiplication operation</em> can be represented graphically as a series of scalar<span class="math notranslate nohighlight">\(\times\)</span>vector multiplication and vector-vector summation operations (<a class="reference internal" href="#fig-right-multiplication-matrix-vector"><span class="std std-numref">Fig. 10</span></a>):</p>
<figure class="align-default" id="fig-right-multiplication-matrix-vector">
<a class="reference internal image-reference" href="../_images/Fig-Ab-Multiplication.pdf"><img alt="../_images/Fig-Ab-Multiplication.pdf" src="../_images/Fig-Ab-Multiplication.pdf" style="height: 140px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Schematic of the right matrix-vector product. The right product, which produces a column vector with the same number of rows as the matrix, can be modeled as a series of scalar<span class="math notranslate nohighlight">\(\times\)</span>vector multiplication and vector-vector summation operations.</span><a class="headerlink" href="#fig-right-multiplication-matrix-vector" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A psuedo code implemetation of <a class="reference internal" href="#defn-right-matrix-vector-multiplication">Definition 11</a> is given in <a class="reference internal" href="#algo-right-multiplication-matrix-vector">Algorithm 3</a>:</p>
<div class="proof algorithm dropdown admonition" id="algo-right-multiplication-matrix-vector">
<p class="admonition-title"><span class="caption-number">Algorithm 3 </span> (Naive right multiplication of a matrix by a vector)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs:</strong> Matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and vector <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{n\times{1}}\)</span></p>
<p><strong>Outputs:</strong> Product vector <span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{m\times{1}}\)</span>.</p>
<p><strong>Initialize</strong></p>
<ol class="simple">
<li><p>set  <span class="math notranslate nohighlight">\((m, n)\leftarrow\text{size}(\mathbf{A})\)</span></p></li>
<li><p>initialize <span class="math notranslate nohighlight">\(\mathbf{y}\leftarrow\text{zeros}(m)\)</span></p></li>
</ol>
<p><strong>Main</strong></p>
<ol class="simple">
<li><p>for <span class="math notranslate nohighlight">\(i\in{1,\dots,m}\)</span>:</p>
<ol class="simple">
<li><p>for <span class="math notranslate nohighlight">\(j\in{1,\dots,n}\)</span>:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(y_{i}~\leftarrow y_{i} + a_{ij}\times{x_{j}}\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Return</strong> vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span></p>
</section>
</div></section>
<section id="left-multiplication-of-a-matrix-by-a-vector">
<h4>Left multiplication of a matrix by a vector<a class="headerlink" href="#left-multiplication-of-a-matrix-by-a-vector" title="Permalink to this headline">#</a></h4>
<p>We could also consider the <em>left multiplication</em> of a matrix by a vector. Suppose <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is an <span class="math notranslate nohighlight">\(m\times{n}\)</span> matrix, and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a <span class="math notranslate nohighlight">\(m\times{1}\)</span> vector, then the <em>left matrix-vector product</em> is a row vector:</p>
<div class="proof definition admonition" id="defn-left-multiply-matrix-vector">
<p class="admonition-title"><span class="caption-number">Definition 12 </span> (Left matrix-vector product)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{m\times{1}}\)</span>. Then, the <em>left matrix-vector product</em>:</p>
<div class="math notranslate nohighlight" id="equation-eqn-left-matrix-vector-product-matrix">
<span class="eqno">(21)<a class="headerlink" href="#equation-eqn-left-matrix-vector-product-matrix" title="Permalink to this equation">#</a></span>\[\mathbf{y} = \mathbf{x}^{T}\mathbf{A}\]</div>
<p>produces the vector <span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{1\times{n}}\)</span> such that:</p>
<div class="math notranslate nohighlight" id="equation-eqn-left-matrix-vector-product-index">
<span class="eqno">(22)<a class="headerlink" href="#equation-eqn-left-matrix-vector-product-index" title="Permalink to this equation">#</a></span>\[y_{i} = \sum_{j=1}^{m}a_{ji}x_{j}\qquad{i=1,2,\dots,n}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}^{T}\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Transpose">transpose</a> of the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
</section>
</div><p>The <em>left multiplication operation</em> can be represented graphically as a series of scalar<span class="math notranslate nohighlight">\(\times\)</span>vector multiplication and vector-vector summation operations (<a class="reference internal" href="#fig-left-multiplication-matrix-vector"><span class="std std-numref">Fig. 11</span></a>):</p>
<figure class="align-default" id="fig-left-multiplication-matrix-vector">
<a class="reference internal image-reference" href="../_images/Fig-bA-Left-Multiplication.pdf"><img alt="../_images/Fig-bA-Left-Multiplication.pdf" src="../_images/Fig-bA-Left-Multiplication.pdf" style="height: 160px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">Schematic of the left matrix-vector product. The left product, which produces a row vector with the same number of columns as the matrix, can be modeled as a series of scalar<span class="math notranslate nohighlight">\(\times\)</span>vector multiplication and vector-vector summation operations.</span><a class="headerlink" href="#fig-left-multiplication-matrix-vector" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A psuedo code implemetation of <a class="reference internal" href="#defn-left-multiply-matrix-vector">Definition 12</a> is given in <a class="reference internal" href="#algo-left-multiplication-matrix-vector">Algorithm 4</a>:</p>
<div class="proof algorithm dropdown admonition" id="algo-left-multiplication-matrix-vector">
<p class="admonition-title"><span class="caption-number">Algorithm 4 </span> (Naive left multiplication of a matrix by a vector)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs:</strong> Matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and vector <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{m\times{1}}\)</span></p>
<p><strong>Outputs:</strong> Product vector <span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{1\times{n}}\)</span>.</p>
<p><strong>Initialize</strong></p>
<ol class="simple">
<li><p>set  <span class="math notranslate nohighlight">\((m, n)\leftarrow\text{size}(\mathbf{A})\)</span></p></li>
<li><p>initialize <span class="math notranslate nohighlight">\(\mathbf{y}\leftarrow\text{zeros}(n)\)</span></p></li>
</ol>
<p><strong>Main</strong></p>
<ol class="simple">
<li><p>for <span class="math notranslate nohighlight">\(i\in{1,\dots,n}\)</span>:</p>
<ol class="simple">
<li><p>for <span class="math notranslate nohighlight">\(j\in{1,\dots,m}\)</span>:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(y_{i}~\leftarrow y_{i} + a_{ji}\times{x_{j}}\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Return</strong> vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span></p>
</section>
</div></section>
<section id="matrix-matrix-products">
<h4>Matrix-Matrix products<a class="headerlink" href="#matrix-matrix-products" title="Permalink to this headline">#</a></h4>
<p>Many of the important uses of matrices in engineering practice depend upon the definition of matrix multiplication.</p>
<div class="proof definition admonition" id="defn-matrix-matrix-product">
<p class="admonition-title"><span class="caption-number">Definition 13 </span> (Matrix-matrix product)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\in\mathbb{R}^{n\times{p}}\)</span>. The matrix-matrix product <span class="math notranslate nohighlight">\(\mathbf{C} = \mathbf{A}\times\mathbf{B}\)</span> produces the matrix <span class="math notranslate nohighlight">\(\mathbf{C}\in\mathbb{R}^{m\times{p}}\)</span> with elements:</p>
<div class="math notranslate nohighlight" id="equation-eqn-matrix-matrix-product-elements">
<span class="eqno">(23)<a class="headerlink" href="#equation-eqn-matrix-matrix-product-elements" title="Permalink to this equation">#</a></span>\[c_{ij} = \sum_{k=1}^{n}a_{ik}b_{kj}\qquad{i=1,2,\cdots,m~\text{and}~j=1,2,\cdots,p}\]</div>
<p>The matrices <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\in\mathbb{R}^{n\times{p}}\)</span> are compatible if the number of columns of <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> equals the number of rows of <span class="math notranslate nohighlight">\(\mathbf{B}\in\mathbb{R}^{n\times{p}}\)</span>. Otherwise, the matrices are incompatible and cannot be multiplied.</p>
<p>The runtime of matrix multiplication is <span class="math notranslate nohighlight">\(\mathcal{O}(n^3)\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the dimension of the matrices being multiplied.</p>
</section>
</div><p>The <em>matrix-matrix multiplication</em> operation can be represented graphically as a series of right matrix-vector products (<a class="reference internal" href="#fig-multiplication-matrix-matrix"><span class="std std-numref">Fig. 12</span></a>):</p>
<figure class="align-default" id="fig-multiplication-matrix-matrix">
<a class="reference internal image-reference" href="../_images/Fig-AB-Matrix-Matrix-Multiplication.pdf"><img alt="../_images/Fig-AB-Matrix-Matrix-Multiplication.pdf" src="../_images/Fig-AB-Matrix-Matrix-Multiplication.pdf" style="height: 380px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">Schematic of matrix-matrix multiplication. This operation, which produces a matrix product, can be modeled as a series of right matrix-vector products.</span><a class="headerlink" href="#fig-multiplication-matrix-matrix" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A psuedo code implemetation of <a class="reference internal" href="#defn-matrix-matrix-product">Definition 13</a> is given in <a class="reference internal" href="#algo-matrix-matrix-code">Algorithm 5</a>:</p>
<div class="proof algorithm dropdown admonition" id="algo-matrix-matrix-code">
<p class="admonition-title"><span class="caption-number">Algorithm 5 </span> (Naive Matrix <span class="math notranslate nohighlight">\(\times\)</span> Matrix multiplication)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span>, and matrix <span class="math notranslate nohighlight">\(\mathbf{B}\in\mathbb{R}^{n\times{p}}\)</span></p>
<p><strong>Outputs</strong> Product matrix <span class="math notranslate nohighlight">\(\mathbf{C}\in\mathbb{R}^{m\times{p}}\)</span>.</p>
<p><strong>Initialize</strong></p>
<ol class="simple">
<li><p>initialize <span class="math notranslate nohighlight">\((m, n)\leftarrow\text{size}(\mathbf{A})\)</span></p></li>
<li><p>initialize <span class="math notranslate nohighlight">\((n, p)\leftarrow\text{size}(\mathbf{B})\)</span></p></li>
<li><p>initialize matrix <span class="math notranslate nohighlight">\(\mathbf{C}\leftarrow\text{zeros}(m, p)\)</span></p></li>
</ol>
<p><strong>Main</strong></p>
<ol class="simple">
<li><p>for <span class="math notranslate nohighlight">\(i\in{1,\dots, m}\)</span></p>
<ol class="simple">
<li><p>for <span class="math notranslate nohighlight">\(j\in{1,\dots,p}\)</span></p>
<ol class="simple">
<li><p>for <span class="math notranslate nohighlight">\(k\in{1,\dots,n}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(c_{ij}~\leftarrow~c_{ij} + a_{ik}\times{b_{kj}}\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Return</strong> matrix <span class="math notranslate nohighlight">\(\mathbf{C}\)</span></p>
</section>
</div><p>Finally, matrix-matrix products have different properties compared with the product of two scalar numbers:</p>
<ul class="simple">
<li><p>Non-commutativity: Matrix multiplication is typically not commutative, e.g., <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{B}\neq\mathbf{B}\mathbf{A}\)</span>.</p></li>
<li><p>Distributivity: Matrix products are distributive, i.e., <span class="math notranslate nohighlight">\(\mathbf{A}\left(\mathbf{B}+\mathbf{C}\right) = \mathbf{A}\mathbf{B}+\mathbf{A}\mathbf{C}\)</span>.</p></li>
<li><p>Associative: Matrix products are associative, i.e., <span class="math notranslate nohighlight">\(\mathbf{A}\left(\mathbf{B}\mathbf{C}\right) = \left(\mathbf{A}\mathbf{B}\right)\mathbf{C}\)</span>.</p></li>
<li><p>Transpose: The transpose of a matrix product is the product of transposes, i.e., <span class="math notranslate nohighlight">\(\left(\mathbf{A}\mathbf{B}\right)^{T} = \mathbf{B}^{T}\mathbf{A}^{T}\)</span>.</p></li>
</ul>
<hr class="docutils" />
</section>
</section>
</section>
<section id="measurements-and-distances">
<span id="content-measurements-distances"></span><h2>Measurements and distances<a class="headerlink" href="#measurements-and-distances" title="Permalink to this headline">#</a></h2>
<section id="vector-and-matrix-norms">
<h3>Vector and matrix norms<a class="headerlink" href="#vector-and-matrix-norms" title="Permalink to this headline">#</a></h3>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norm</a> is a function that measures the length of vectors or matrices. The notion of length is handy because it enables us to define distance, i.e., similarity between vectors (or matrices) in applications such as machine learning.</p>
<p>A vector norm is any function <span class="math notranslate nohighlight">\(||\star||:\mathbb{R}^{n}\rightarrow\mathbb{R}\)</span> such that following properties are true:</p>
<ul class="simple">
<li><p>Non-negativity: <span class="math notranslate nohighlight">\(||\mathbf{x}||\geq{0}\)</span> for any vector <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{n}\)</span></p></li>
<li><p>Multiplication by a scalar: <span class="math notranslate nohighlight">\(||\alpha\mathbf{x}|| = \alpha{||\mathbf{x}||}\)</span> for any vector <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{n}\)</span> and <span class="math notranslate nohighlight">\(\alpha\in\mathbb{R}\)</span>.</p></li>
<li><p>Triangle inequality: <span class="math notranslate nohighlight">\(||\mathbf{x}+\mathbf{x}||\leq||\mathbf{x}||+||\mathbf{x}||\)</span> for any vectors <span class="math notranslate nohighlight">\(\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}\)</span></p></li>
</ul>
<div class="proof definition admonition" id="defn-vector-p-norm">
<p class="admonition-title"><span class="caption-number">Definition 14 </span> (p-norm)</p>
<section class="definition-content" id="proof-content">
<p>Arguably, the most commonly used vector norms belong to the family of <span class="math notranslate nohighlight">\(p\)</span>-norms (also called <span class="math notranslate nohighlight">\(l_{p}\)</span>-norms) which is defined as:</p>
<div class="math notranslate nohighlight" id="equation-eqn-p-norm-defn">
<span class="eqno">(24)<a class="headerlink" href="#equation-eqn-p-norm-defn" title="Permalink to this equation">#</a></span>\[||\mathbf{x}||_{p} = \left(\sum_{i=1}^{n}|x_{i}|^{p}\right)^{1/p}\]</div>
<p>for any <span class="math notranslate nohighlight">\(p&gt;0\)</span>.</p>
</section>
</div><p>A matrix norm is any function <span class="math notranslate nohighlight">\(||\star||:\mathbb{R}^{m\times{n}}\rightarrow\mathbb{R}\)</span> such that following properties are true:</p>
<ul class="simple">
<li><p>Non-negativity: <span class="math notranslate nohighlight">\(||\mathbf{A}||\geq{0}\)</span> for any matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and <span class="math notranslate nohighlight">\(||\mathbf{A}||=0\)</span> if and only if <span class="math notranslate nohighlight">\(\mathbf{A}=0\)</span>.</p></li>
<li><p>Multiplication by a scalar: <span class="math notranslate nohighlight">\(||\alpha\mathbf{A}|| = \alpha{||\mathbf{A}||}\)</span> for any <span class="math notranslate nohighlight">\(m\times{n}\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and <span class="math notranslate nohighlight">\(\alpha\in\mathbb{R}\)</span>.</p></li>
<li><p>Triangle inequality: <span class="math notranslate nohighlight">\(||\mathbf{A}+\mathbf{B}||\leq||\mathbf{A}||+||\mathbf{B}||\)</span> for any matrices <span class="math notranslate nohighlight">\(\mathbf{A},\mathbf{B}\in\mathbb{R}^{m\times{n}}\)</span></p></li>
</ul>
<div class="proof definition admonition" id="defn-compatible-matrix-vector-norm">
<p class="admonition-title"><span class="caption-number">Definition 15 </span> (Properties of Matrix Norms)</p>
<section class="definition-content" id="proof-content">
<p>A matrix norm <span class="math notranslate nohighlight">\(||\cdot||\)</span> is <em>consistent</em> with a vector norm <span class="math notranslate nohighlight">\(||\cdot||\)</span> if:</p>
<div class="math notranslate nohighlight" id="equation-eqn-consistent-matrix-norm">
<span class="eqno">(25)<a class="headerlink" href="#equation-eqn-consistent-matrix-norm" title="Permalink to this equation">#</a></span>\[||\mathbf{A}\mathbf{x}||\leq||\mathbf{A}||\cdot||\mathbf{x}||\]</div>
<p>Further, a matrix norm <span class="math notranslate nohighlight">\(||\cdot||\)</span> is <em>sub-multiplicative</em> if <span class="math notranslate nohighlight">\(\forall\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> and
<span class="math notranslate nohighlight">\(\forall\mathbf{B}\in\mathbb{R}^{n\times{q}}\)</span> the inequality holds:</p>
<div class="math notranslate nohighlight" id="equation-eqn-submul-norm">
<span class="eqno">(26)<a class="headerlink" href="#equation-eqn-submul-norm" title="Permalink to this equation">#</a></span>\[||\mathbf{A}\mathbf{B}||\leq||\mathbf{A}||\cdot||\mathbf{B}||\]</div>
</section>
</div></section>
<section id="other-distance-metrics">
<h3>Other distance metrics<a class="headerlink" href="#other-distance-metrics" title="Permalink to this headline">#</a></h3>
<p>Other functions can be used to measure the similarity (or distance between) vectors and matrices:</p>
<div class="proof definition admonition" id="defn-rbf-measure">
<p class="admonition-title"><span class="caption-number">Definition 16 </span> (Radial basis function)</p>
<section class="definition-content" id="proof-content">
<p>A radial basis function (RBF) measures the similarity between two input vectors <span class="math notranslate nohighlight">\(\mathbf{x}_{p}\in\mathbb{R}^{m\times{1}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x_{q}}\in\mathbb{R}^{m\times{1}}\)</span>. The most common is the Gaussian radial basis function, defined by a bell-shaped curve:</p>
<div class="math notranslate nohighlight" id="equation-eqn-rbf-similarity-function">
<span class="eqno">(27)<a class="headerlink" href="#equation-eqn-rbf-similarity-function" title="Permalink to this equation">#</a></span>\[k\left(\mathbf{x}_{p},\mathbf{x}_{q}\right) = \sigma_{f}^{2}\exp\left(-\frac{1}{2}\left(\mathbf{x}_{p} - \mathbf{x}_{q}\right)^{T}\mathbf{M}
\left(\mathbf{x}_{p} - \mathbf{x}_{q}\right)\right) +\sigma_{n}^{2}\delta_{pq}\]</div>
<p>where the matrix <span class="math notranslate nohighlight">\(\mathbf{M}\in\mathbb{R}^{m\times{m}}\)</span> can be any symmetric matrix, <span class="math notranslate nohighlight">\(\sigma_{f}^{2}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{n}^{2}\)</span> are constants and <span class="math notranslate nohighlight">\(\delta_{pq}\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a>.</p>
</section>
</div><p>We shall see that radial basis functions are used in various machine learning applications, such as classification.</p>
</section>
</section>
<section id="dimensionality-reduction">
<span id="content-dimensionality-reduction"></span><h2>Dimensionality reduction<a class="headerlink" href="#dimensionality-reduction" title="Permalink to this headline">#</a></h2>
<p>Dimensionality reduction systematically reduces the number of variables in a dataset while preserving as much of the information in the data as possible. Dimensionality reduction simplifies data, removes noise, and makes patterns in the data more visible. It can also help visualize data, improve machine learning algorithms’ performance, and reduce the storage and computational requirements of working with large datasets.</p>
<p>There are many techniques for dimensionality reduction, including <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a>, which are both based on solving <a class="reference internal" href="#content-eigenvalue-eigenvector-problems"><span class="std std-ref">Eigenvalue-eigenvector problems</span></a>. Other approaches, such as clustering and <a class="reference external" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-distributed stochastic neighbor embedding (t-SNE)</a>, are based upon minimizing some other distance measure.</p>
<section id="eigenvalue-eigenvector-problems">
<span id="content-eigenvalue-eigenvector-problems"></span><h3>Eigenvalue-eigenvector problems<a class="headerlink" href="#eigenvalue-eigenvector-problems" title="Permalink to this headline">#</a></h3>
<p>Eigenvalue-eigenvector problems are a type of mathematical problem that involves finding a set of scalar values <span class="math notranslate nohighlight">\(\left\{\lambda_{1},\dots,\lambda_{m}\right\}\)</span> called <a class="reference external" href="https://mathworld.wolfram.com/Eigenvalue.html">eigenvalues</a> and a set of vectors <span class="math notranslate nohighlight">\(\left\{\mathbf{v}_{1},\dots,\mathbf{v}_{m}\right\}\)</span> called <a class="reference external" href="https://mathworld.wolfram.com/Eigenvector.html">eigenvectors</a> such that:</p>
<div class="math notranslate nohighlight" id="equation-eqn-eigenvalue-eigenvector-problem">
<span class="eqno">(28)<a class="headerlink" href="#equation-eqn-eigenvalue-eigenvector-problem" title="Permalink to this equation">#</a></span>\[\mathbf{A}\mathbf{v}_{j} = \lambda_{j}\mathbf{v}_{j}\qquad{j=1,2,\dots,m}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{m}}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}\in\mathbb{R}^{m\times{1}}\)</span> is a column vector, and <span class="math notranslate nohighlight">\(\lambda\in\mathbb{R}\)</span> is a scalar. Eigenvalues and eigenvectors are used in many areas of mathematics, engineering, and physics, including image compression and data reduction approaches such as <a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a>.</p>
<p>In addition to thier other uses, eigenvalues have a another interesting feature (<a class="reference internal" href="#obs-eigenvalues-determinants">Observation 3</a>):</p>
<div class="proof observation admonition" id="obs-eigenvalues-determinants">
<p class="admonition-title"><span class="caption-number">Observation 3 </span> (Determinants and eigenvalues)</p>
<section class="observation-content" id="proof-content">
<p>Eigenvalues can be used directly to calculate the determinant of a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{m}}\)</span>. Denote the set of
eignenvalues for the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{m}}\)</span> as <span class="math notranslate nohighlight">\(\left\{\lambda_{1},\dots,\lambda_{m}\right\}\)</span>. Then, the <span class="math notranslate nohighlight">\(\det\left(\mathbf{A}\right)\)</span> is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-det-a-eigenvalues">
<span class="eqno">(29)<a class="headerlink" href="#equation-eqn-det-a-eigenvalues" title="Permalink to this equation">#</a></span>\[\det\left(\mathbf{A}\right) = \prod_{i=1}^{m}\lambda_{i}\]</div>
<p>A matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{m}}\)</span> is non-singular if <span class="math notranslate nohighlight">\(\lambda_{i}&gt;0~\forall{i}\)</span>, otherwise it is singular.</p>
</section>
</div></section>
<section id="singular-value-decomposition">
<h3>Singular value decomposition<a class="headerlink" href="#singular-value-decomposition" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular value decomposition (SVD)</a> is a powerful tool used in many applications, such as image and data compression, signal processing, and machine learning. SVD factors a matrix into a canonical form composed of an orthogonal matrix, a diagonal matrix, and another orthogonal matrix:</p>
<div class="proof definition admonition" id="defn-svd-real-matrix">
<p class="admonition-title"><span class="caption-number">Definition 17 </span> (Singular value decomposition)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span>. The singular value decomposition of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-math-svd">
<span class="eqno">(30)<a class="headerlink" href="#equation-eqn-math-svd" title="Permalink to this equation">#</a></span>\[\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> are orthogonal matrices and <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> is a diagonal matrix containing the singular values <span class="math notranslate nohighlight">\(\sigma_{i}=\Sigma_{ii}\)</span> along the main diagonal.</p>
</section>
</div><p>SVD can be used to diagonalize a matrix, find the eigenvalues and eigenvectors of a matrix, and solve linear equations. It is also essential in <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis (PCA)</a> as a dimensionality reduction technique.</p>
<div class="proof observation admonition" id="obs-svd-matrix-decomposition">
<p class="admonition-title"><span class="caption-number">Observation 4 </span> (SVD matrix decomposition)</p>
<section class="observation-content" id="proof-content">
<p>The singular value decomposition (SVD) can be thought of as decomposing a matrix into a weighted, ordered sum of separable matrices.
Let <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> have the singular value decomposition <span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}\)</span>.</p>
<p>Then, the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in\mathbb{R}^{m\times{n}}\)</span> can be written as:</p>
<div class="math notranslate nohighlight" id="equation-eqn-matrix-decomp">
<span class="eqno">(31)<a class="headerlink" href="#equation-eqn-matrix-decomp" title="Permalink to this equation">#</a></span>\[\mathbf{A} = \sum_{i=1}^{R_{\mathbf{A}}}\sigma_{i}\left(\mathbf{u}_{i}\otimes\mathbf{v}_{i}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(R_{\mathbf{A}}\)</span> denotes the rank of matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, the vectors <span class="math notranslate nohighlight">\(\mathbf{u}_{i}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_{i}\)</span> are the ith columns of the corresponding SVD matrices, and <span class="math notranslate nohighlight">\(\sigma_{i}\)</span> are the ordered singular values.</p>
<p>The outer-product <span class="math notranslate nohighlight">\(\left(\mathbf{u}_{i}\otimes\mathbf{v}_{i}\right)\)</span> is the separable component of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>.</p>
</section>
</div><section id="connection-of-svd-and-eigendecomposition">
<h4>Connection of SVD and eigendecomposition<a class="headerlink" href="#connection-of-svd-and-eigendecomposition" title="Permalink to this headline">#</a></h4>
<p>Fill me in.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h2>
<p>This lecture introduced vectors, matrices, and operations defined on these objects:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-references-matrix-vector"><span class="std std-ref">Matricies and vectors</span></a> are one- and two-dimensional arrays of numbers. Vectors represent quantities with both magnitude and direction, such as displacement, velocity, and acceleration. They can also describe points in space or as coefficients in linear equations. Matrices are represented as a grid of numbers, with each matrix element represented by a different cell in the grid. Matrices are often used to describe linear transformations, such as rotations and scaling operations, as well as to represent systems of linear equations. They can also represent data sets, with each row representing a different data point and each column representing a distinct feature.</p></li>
<li><p><a class="reference internal" href="#content-measurements-distances"><span class="std std-ref">Measurements and distances</span></a> are tools to measure distances between matrix and vector objects.</p></li>
<li><p><a class="reference internal" href="#content-dimensionality-reduction"><span class="std std-ref">Dimensionality reduction</span></a> systematically reduces the number of variables in a dataset while preserving as much information as possible. Dimensionality reduction simplifies data, removes noise, and makes patterns in the data more visible. It can also help visualize data, improve machine learning algorithms’ performance, and reduce the storage and computational requirements of working with large datasets.</p></li>
</ul>
</section>
<section id="additonal-resources">
<h2>Additonal resources<a class="headerlink" href="#additonal-resources" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>The matrix vector and matrix <span class="math notranslate nohighlight">\(\times\)</span> matrix product figures were inspired <a class="reference external" href="https://dzone.com/articles/visualizing-matrix">Visualizing Matrix Multiplication as a Linear Combination, Eli Bendersky, Apr. 12, 15 · Big Data Zone</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./unit-2-data"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="trees.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Data Structures</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="laes.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Linear Algebraic Equations</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Varner<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>